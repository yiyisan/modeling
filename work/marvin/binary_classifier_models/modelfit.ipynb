{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from enum import Enum\n",
    "import logging\n",
    "\n",
    "try:\n",
    "    from exceptions import Exception\n",
    "except:\n",
    "    pass\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "class BinaryClassifier(Enum):\n",
    "    GBM = 'GBM'\n",
    "    XGB = 'XGBOOST'\n",
    "    LGB = 'LightGBM'\n",
    "    RF = 'RF'\n",
    "    LR = 'LR'\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        if self is BinaryClassifier.GBM:\n",
    "            import work.marvin.binary_classifier_models.bestGbdtModelProducer\n",
    "            return work.marvin.binary_classifier_models.bestGbdtModelProducer\n",
    "        elif self is BinaryClassifier.XGB:\n",
    "            import work.marvin.binary_classifier_models.bestXgboostModelProducer\n",
    "            return work.marvin.binary_classifier_models.bestXgboostModelProducer\n",
    "        elif self is BinaryClassifier.LGB:\n",
    "            import work.marvin.binary_classifier_models.bestLightgbmModelProducer\n",
    "            return work.marvin.binary_classifier_models.bestLightgbmModelProducer\n",
    "        elif self is BinaryClassifier.RF:\n",
    "            import work.marvin.binary_classifier_models.bestRfModelProducer\n",
    "            return work.marvin.binary_classifier_models.bestRfModelProducer\n",
    "        elif self is BinaryClassifier.LR:\n",
    "            import work.marvin.binary_classifier_models.bestLrModelProducer\n",
    "            return work.marvin.binary_classifier_models.bestLrModelProducer\n",
    "\n",
    "    def produceBestModel(self, traindf, datamapper, target, fig_path=None,\n",
    "                         callbacks=[], verbose=0):\n",
    "        \"\"\"\n",
    "        auto classifier model generation, 3 steps:\n",
    "        1. estimate optimal model parameters space for gridsearch,\n",
    "           depends on sample size and feature size\n",
    "        2. run gridsearch to find best parameter set\n",
    "        3. train the best model using the best parameter set\n",
    "        \"\"\"\n",
    "        # estimate optimal parameters grid space\n",
    "        configspace = self.parameterGrid(datamapper)\n",
    "        try:\n",
    "            return self.model.produceBestModel(traindf,\n",
    "                                               datamapper,\n",
    "                                               target,\n",
    "                                               configspace,\n",
    "                                               fig_path=fig_path,\n",
    "                                               verbose=verbose)\n",
    "        except Exception as e:\n",
    "            logger.error(\"search {} with {} error: {}\".format(self, datamapper, e))\n",
    "            raise\n",
    "\n",
    "    def optimizeBestModel(self, traindf, datamapper, target,\n",
    "                          configspace_manual=None,\n",
    "                          configspace=None,\n",
    "                          score='roc_auc',\n",
    "                          test_metric=roc_auc_score,\n",
    "                          fig_path=None,\n",
    "                          search_alg='GP', n_calls=100,\n",
    "                          seed=27, callbacks=[], verbose=0):\n",
    "        \"\"\"\n",
    "        auto classifier model generation, 3 steps:\n",
    "        1. estimate optimal model parameters space for hyperparam search,\n",
    "           depends on sample size and feature size\n",
    "        2. run hyperparam search to find best parameter set\n",
    "        3. train the best model using the best parameter set\n",
    "        \"\"\"\n",
    "        if configspace_manual is None:\n",
    "            configspace_manual = self.configspace_manual()\n",
    "        if configspace is None:\n",
    "            configspace = self.configspace(datamapper, configspace_manual)\n",
    "        optimize_method = HyperOpt(search_alg).search\n",
    "        try:\n",
    "            return self.model.optimizeBestModel(traindf,\n",
    "                                                datamapper,\n",
    "                                                target,\n",
    "                                                configspace,\n",
    "                                                optimize_method,\n",
    "                                                score=score,\n",
    "                                                test_metric=test_metric,\n",
    "                                                fig_path=fig_path,\n",
    "                                                n_calls=n_calls,\n",
    "                                                verbose=verbose,\n",
    "                                                seed=seed)\n",
    "        except Exception as e:\n",
    "            logger.error(\"optimize {} with {} error: {}\".format(self, search_alg, e))\n",
    "            raise\n",
    "\n",
    "    def parameterGrid(self, datamapper):\n",
    "        if self is BinaryClassifier.LR:\n",
    "            return [{'penalty': ['l1', 'l2']}]\n",
    "        else:\n",
    "            param_grid = self.model.parameterGridInitialization(datamapper.shape)\n",
    "            return [param_grid] if self is BinaryClassifier.RF else param_grid\n",
    "\n",
    "    def configspace(self, datamapper, configspace_manual=None):\n",
    "        if self is BinaryClassifier.LR:\n",
    "            return {'penalty': ['l1', 'l2']}\n",
    "        else:\n",
    "            return self.model.configSpaceInitialization(datamapper.shape, configspace_manual)\n",
    "\n",
    "    def configspace_manual(self):\n",
    "        return self.model.configSpaceManualInitialization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "class HyperOpt(Enum):\n",
    "    GP = \"GP\"\n",
    "    RF = \"RF\"\n",
    "    GBRT = \"GBRT\"\n",
    "\n",
    "    @property\n",
    "    def method(self):\n",
    "        if self is HyperOpt.GP:\n",
    "            from skopt import gp_minimize\n",
    "            return gp_minimize\n",
    "        elif self is HyperOpt.RF:\n",
    "            from skopt import forest_minimize\n",
    "            return forest_minimize\n",
    "        elif self is HyperOpt.GBRT:\n",
    "            from skopt import gbrt_minimize\n",
    "            return gbrt_minimize\n",
    "\n",
    "    def search(self, X_train, y_train,\n",
    "               model_class, param_grid,\n",
    "               score='roc_auc',\n",
    "               n_calls=100,\n",
    "               verbose=0):\n",
    "        \"\"\"\n",
    "        General method for applying `skopt_method` to the data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : np.array\n",
    "            The design matrix, dimension `(n_samples, n_features)`.\n",
    "\n",
    "        y_train : list or np.array\n",
    "            The target, of dimension `n_samples`.\n",
    "\n",
    "        model_class : classifier\n",
    "            A classifier model in the mode of `sklearn`, with at least\n",
    "            `fit` and `predict` methods operating on things like\n",
    "            `X` and `y`.\n",
    "\n",
    "        param_grid : dict\n",
    "            Map from parameter names to pairs of values specifying the\n",
    "            upper and lower ends of the space from which to sample.\n",
    "            The values can also be directly specified as `skopt`\n",
    "            objects like `Categorical`.\n",
    "\n",
    "        score : function or string\n",
    "            An appropriate score function or string recognizable by\n",
    "            sklearn.model_selection.cross_val_score. In sklearn, scores\n",
    "            should be positive and we are minimizing so we always want\n",
    "            smaller to mean better.\n",
    "\n",
    "        n_calls : int\n",
    "            Number of evaluations to do.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of dict\n",
    "            Each has keys 'score' and 'params', where 'params' stores the\n",
    "            values from `param_grid` for that run. The primary organizing\n",
    "            value is 'score'.\n",
    "        Example\n",
    "        -------\n",
    "        >>> skopt_grid = {\n",
    "                'max_depth': (4, 12),\n",
    "                'learning_rate': (0.01, 0.5),\n",
    "                'n_estimators': (20, 200),\n",
    "                'objective' : Categorical(('multi:softprob',)),\n",
    "                'gamma': (0, 0.5),\n",
    "                'min_child_weight': (1, 5),\n",
    "                'subsample': (0.1, 1),\n",
    "                'colsample_bytree': (0.1, 1)}\n",
    "        >>> res = HyperOpt('RF').search(X, y, XGBClassifier, skopt_grid, LOG_SCORE, n_calls=10)\n",
    "\n",
    "        To be followed by (see below):\n",
    "\n",
    "        >>> best_params, best_loss = best_results(res)\n",
    "        \"\"\"\n",
    "        logger.debug(\"********************  HyperOpt start ********************\")\n",
    "        param_keys, param_vecs = zip(*param_grid.items())\n",
    "        param_keys = list(param_keys)\n",
    "        param_vecs = list(param_vecs)\n",
    "\n",
    "        def skopt_scorer(param_vec):\n",
    "            params = dict(zip(param_keys, param_vec))\n",
    "            logger.info(params)\n",
    "            err = cross_validated_scorer(\n",
    "                X_train, y_train, model_class, params, score, verbose=verbose)\n",
    "            return err\n",
    "\n",
    "        try:\n",
    "            outcome = self.method(skopt_scorer, list(param_vecs), n_calls=n_calls)\n",
    "            results = []\n",
    "            for err, param_vec in zip(outcome.func_vals, outcome.x_iters):\n",
    "                params = dict(zip(param_keys, param_vec))\n",
    "                results.append({'loss': err, 'params': params})\n",
    "                if verbose > 0:\n",
    "                    logger.debug({'loss': err, 'params': params})\n",
    "\n",
    "            logger.debug(\"********************  HyperOpt end ********************\")\n",
    "        except Exception as e:\n",
    "            logger.error(e)\n",
    "            raise\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def prepareDataforTraining(transformed, datamapper=None, train_size=0.75):\n",
    "    \"\"\"\n",
    "    shortcut for train_test_split, currently a simple wrapper for sklearn function\n",
    "    Parameters\n",
    "    ----------\n",
    "    transformed : DataFrame or np.array\n",
    "        The design matrix, dimension `(n_samples, n_features)`.\n",
    "\n",
    "    datamapper : sklearn-pandas DataFrameMapper, currently not used\n",
    "\n",
    "    train_size : ratio of train / totalset\n",
    "    \"\"\"\n",
    "    traindf, testdf = train_test_split(transformed, train_size=train_size)\n",
    "    return traindf, testdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pipelineTrainEvaluation(pipeline, train, target):\n",
    "    \"\"\"\n",
    "    using predefined pipeline to fit, evaluate, score\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pipeline : PMMLPipeLine, a valid mapper to PMML\n",
    "    train    : DataFrame, training data object\n",
    "    target   ï¼šstring, name of target column\n",
    "    \"\"\"\n",
    "    pipeline.fit(train[train.columns.difference([target])], train[target])\n",
    "\n",
    "    prediction = pipeline.predict_proba(train[train.columns.difference([target])])\n",
    "    predprob = pd.DataFrame(prediction[:, 1], columns=['predprob'])\n",
    "    predprob['ytrue'] = train[target].values\n",
    "\n",
    "    auc = roc_auc_score(y_true=predprob['ytrue'], y_score=predprob['predprob'])\n",
    "    cv_score = pipeline.cross_val_score(train[train.columns.difference([target])],\n",
    "                                        train[target], scoring='roc_auc')\n",
    "\n",
    "    return pipeline, predprob, auc, cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def run_experiments(\n",
    "        experimental_run,\n",
    "        trainX,\n",
    "        trainY,\n",
    "        model_class,\n",
    "        score='roc_auc',\n",
    "        test_metric=roc_auc_score,\n",
    "        n_folds=1,\n",
    "        verbose=0,\n",
    "        random_state=None,\n",
    "        dataset_name=None):\n",
    "    \"\"\"\n",
    "    Basic experimental framework.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    experimental_run : list of tuples\n",
    "        These tuples should have exactly three members: the first one\n",
    "        of `grid_search`, `randomized_search`, `hyperopt_search`,\n",
    "        `skopt_gp_minimize`, `skopt_forest_minimize`, or\n",
    "        `skopt_forest_gbrt`, the second an appropriate `param_grid`\n",
    "        dict for that function, and the third a dict specifying\n",
    "        keyword arguments to the search function.\n",
    "\n",
    "    dataset : (np.array, iterable)\n",
    "        A dataset (X, y) where `X` has dimension\n",
    "        `(n_samples, n_features)` and `y` has\n",
    "         dimension `n_samples`.\n",
    "\n",
    "    model_class : classifier\n",
    "        A classifier model in the mode of `sklearn`, with at least\n",
    "        `fit` and `predict` methods operating on things like\n",
    "        `X` and `y`.\n",
    "\n",
    "    score : function or string\n",
    "        An appropriate score function or string recognizable by\n",
    "        `sklearn.model_selection.cross_val_score`. In `sklearn`, scores\n",
    "        are positive and we are maximizing so we always want higher to mean\n",
    "        better.\n",
    "\n",
    "    test_metric : function\n",
    "        An `sklearn.metrics` function.\n",
    "\n",
    "    random_state : int\n",
    "\n",
    "    dataset_name : str or None\n",
    "        Informal name to give the dataset. Purely for\n",
    "        book-keeping.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of dict\n",
    "       Each dict is a results dictionary of the sort returned\n",
    "       by `assess`.\n",
    "    \"\"\"\n",
    "    X = trainX\n",
    "    y = trainY\n",
    "    if n_folds <= 1:\n",
    "        skf = ((X, y, X, y),)\n",
    "    else:\n",
    "        skf = get_cross_validation_split(X, y,\n",
    "                                         n_folds=n_folds,\n",
    "                                         random_state=random_state)\n",
    "\n",
    "    all_results = []\n",
    "    # This loop can easily be parallelized, but doing so can\n",
    "    # be tricky on some systems, since `cross_val_score`\n",
    "    # calls `joblib` even if `n_jobs=1`, resulting in\n",
    "    # nested parallel jobs even if there is no actual\n",
    "    # parallelization elsewhere in the experimental run\n",
    "    for search_func, param_grid, kwargs in experimental_run:\n",
    "        all_results.append(\n",
    "            assess(\n",
    "                skf,\n",
    "                search_func=search_func,\n",
    "                model_class=model_class,\n",
    "                param_grid=param_grid,\n",
    "                score=score,\n",
    "                test_metric=test_metric,\n",
    "                dataset_name=dataset_name,\n",
    "                search_func_args=kwargs,\n",
    "                verbose=verbose))\n",
    "        logger.info(\"******************** assess end *******************\")\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def assess(\n",
    "        cv_split,\n",
    "        search_func,\n",
    "        model_class,\n",
    "        param_grid,\n",
    "        n_folds=1,\n",
    "        score='roc_auc',\n",
    "        test_metric=roc_auc_score,\n",
    "        dataset_name=None,\n",
    "        search_func_args={},\n",
    "        callbacks=[],\n",
    "        verbose=0):\n",
    "    \"\"\"\n",
    "    The core of the experimental framework. This runs cross-validation\n",
    "    and, for the inner loop, does cross-validation to find the optimal\n",
    "    hyperparameters according to `search_func`. These optimal\n",
    "    parameters are then used for an assessment in the outer\n",
    "    cross-validation run.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cv_split: generator of 4-tuple of (X_train, y_train, X_test, y_test)\n",
    "\n",
    "    search_func : function\n",
    "        The search function to use. Can be `grid_search`,\n",
    "        `randomized_search`, `hyperopt_search`, `skopt_gp_minimize`,\n",
    "        `skopt_forest_minimize`, or `skopt_forest_gbrt`, all\n",
    "        defined in this module. This choice has to be compatible with\n",
    "        `param_grid`, in the sense that `grid_search` and\n",
    "        `randomized_search` require a dict from strings to lists of\n",
    "        values, `hyperopt_search` requires a dict from strings to\n",
    "        hyperopt sampling functions, and the `skopt` functions\n",
    "        require dicts from strings to (upper, lower) pairs of\n",
    "        special `skopt` functions.\n",
    "\n",
    "    model_class : classifier\n",
    "        A classifier model in the mode of `sklearn`, with at least\n",
    "        `fit` and `predict` methods operating on things like\n",
    "        `X` and `y`.\n",
    "\n",
    "    param_grid : dict\n",
    "        Map from parameter names to appropriate specifications of\n",
    "        appropriate values for that parameter. This is not the\n",
    "        expanded grid, but rather the simple map that can be expanded\n",
    "        by `expand_grid` below (though not all methods call for that).\n",
    "        This has to be compatible with  `search_func`, and all the\n",
    "        values must be suitable arguments to `model_class` instances.\n",
    "\n",
    "    score : function or string\n",
    "        An appropriate score function or string recognizable by\n",
    "        `sklearn.model_selection.cross_val_score`. In `sklearn`, scores\n",
    "        are positive and we are maximizing so we always want higher to mean\n",
    "        better.\n",
    "\n",
    "    test_metric : function\n",
    "        An `sklearn.metrics` function.\n",
    "\n",
    "    xval_indices : list\n",
    "        List of train and test indices into `X` and `y`. This defines\n",
    "        the cross-validation. This is done outside of this method to\n",
    "        allow for identical splits across different experiments.\n",
    "\n",
    "    dataset_name : str or None\n",
    "        Name for the dataset being analyzed. For book-keeping and\n",
    "        display only.\n",
    "\n",
    "    search_func_args : dict\n",
    "        Keyword arguments to feed to `search_func`.\n",
    "\n",
    "    callbacks: list\n",
    "        List of all callback func following the sklearn idioms\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Accumulated information about the experiment:\n",
    "\n",
    "        {'Test accuracy': list of float,\n",
    "         'Cross-validation time':list of float,\n",
    "         'Parameters sampled': list of int,\n",
    "         'Iteration details': list of list,\n",
    "         'Method': search_func.__name__,\n",
    "         'Model': model_class.__name__,\n",
    "         'Dataset': dataset_name,\n",
    "         'Best parameters': list of dict,\n",
    "         'Mean test accuracy': float,\n",
    "         'Mean cross-validation time': float,\n",
    "         'Mean parameters sampled': float}\n",
    "    \"\"\"\n",
    "    logger.info(search_func)\n",
    "    logger.info(model_class)\n",
    "\n",
    "    data = {'Test accuracy': [],\n",
    "            'Cross-validation time': [],\n",
    "            'Parameters sampled': [],\n",
    "            'Iteration details': [],\n",
    "            'Method': search_func.__name__,\n",
    "            'Model': model_class.__name__,\n",
    "            'Dataset': dataset_name,\n",
    "            'Best parameters': []}\n",
    "\n",
    "    for X_train, y_train, X_test, y_test in cv_split:\n",
    "        start = time()\n",
    "        results = search_func(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            model_class,\n",
    "            param_grid,\n",
    "            score,\n",
    "            verbose=verbose,\n",
    "            **search_func_args)\n",
    "\n",
    "        best_params = sorted(results, key=itemgetter('loss'), reverse=True)\n",
    "        best_params = best_params[0]['params']\n",
    "        data['Best parameters'].append(best_params)\n",
    "        bestmod = model_class(**best_params)\n",
    "        bestmod.fit(X_train, y_train)\n",
    "        predictions = bestmod.predict(X_train)\n",
    "        data['Cross-validation time'].append(time() - start)\n",
    "        data['Parameters sampled'].append(len(results))\n",
    "        data['Iteration details'].append(results)\n",
    "        data['Test accuracy'].append(test_metric(y_test, predictions))\n",
    "\n",
    "    data['Mean test accuracy'] = np.mean(data['Test accuracy'])\n",
    "    data['Mean cross-validation time'] = np.mean(data['Cross-validation time'])\n",
    "    data['Mean parameters sampled'] = np.mean(data['Parameters sampled'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def searchBestParamsSkopt(train, labels_train, skopt_grid, search_alg, train_alg,\n",
    "                          score='roc_auc',\n",
    "                          test_metric=roc_auc_score,\n",
    "                          n_folds=1,\n",
    "                          verbose=0, n_calls=100):\n",
    "    experiment_setting = [(search_alg, skopt_grid, {'n_calls': n_calls})]\n",
    "\n",
    "    experiment_result = run_experiments(experiment_setting,\n",
    "                                        train,\n",
    "                                        labels_train,\n",
    "                                        train_alg,\n",
    "                                        score=score,\n",
    "                                        test_metric=test_metric,\n",
    "                                        n_folds=n_folds,\n",
    "                                        verbose=verbose)\n",
    "\n",
    "    test_accuracy = experiment_result[0]['Test accuracy']\n",
    "    max_index = test_accuracy.index(max(test_accuracy))\n",
    "    best_params = experiment_result[0]['Best parameters'][max_index]\n",
    "    trace = experiment_result[0]['Iteration details'][max_index]\n",
    "    return best_params, trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def get_cross_validation_split(X, y, n_folds=5, random_state=None):\n",
    "    \"\"\"\n",
    "    Use `StratifiedKFold` to create an `n_folds` cross-validator for\n",
    "    the dataset defined by `X` and y`. Only `y` is used, but both are\n",
    "    given for an intuitive interface; `X` could just as easily be used.\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_folds, random_state=random_state)\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        if hasattr(X, 'iloc'):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        else:\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "        if hasattr(y, 'iloc'):\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        else:\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "    yield X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def cross_validated_scorer(\n",
    "        X_train, y_train, model_class, params, score, verbose=0, kfolds=5):\n",
    "    \"\"\"\n",
    "    The scoring function used through this module, by all search\n",
    "    functions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : np.array\n",
    "        The design matrix, dimension `(n_samples, n_features)`.\n",
    "\n",
    "    y_train : list or np.array\n",
    "        The target, of dimension `n_samples`.\n",
    "\n",
    "    model_class : classifier\n",
    "        A classifier model in the mode of `sklearn`, with at least\n",
    "        `fit` and `predict` methods operating on things like\n",
    "        `X` and `y`.\n",
    "\n",
    "    params : dict\n",
    "        Map from parameter names to single appropriate values\n",
    "        for that parameter. This will be used to build a model\n",
    "        from `model_class`.\n",
    "\n",
    "    score : function or string\n",
    "        An appropriate score function or string recognizable by\n",
    "        `sklearn.model_selection.cross_val_score`. In `sklearn`, scores\n",
    "        are positive and we are maximizing so we always want higher to mean\n",
    "        better.\n",
    "\n",
    "    kfolds : int\n",
    "        Number of cross-validation runs to do.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "       Average loss over the `kfolds` runs.\n",
    "    \"\"\"\n",
    "    if verbose > 0:\n",
    "        logger.info(\"*********************** params **********************\")\n",
    "        logger.info(params)\n",
    "\n",
    "    mod = model_class(**params)\n",
    "    cv_score = cross_val_score(\n",
    "        mod,\n",
    "        X_train,\n",
    "        y=y_train,\n",
    "        scoring=score,\n",
    "        cv=kfolds,\n",
    "        verbose=verbose,\n",
    "        n_jobs=1).mean()\n",
    "\n",
    "    if verbose > 0:\n",
    "        logger.info(\"********************** cv_score *********************\")\n",
    "        logger.info(cv_score)\n",
    "\n",
    "    return cv_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
