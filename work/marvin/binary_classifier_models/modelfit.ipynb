{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# <api>\n",
    "from time import time\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import cross_validation  # Additional scklearn functions\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from enum import Enum\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "try:\n",
    "    from exceptions import Exception\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BinaryClassifier(Enum):\n",
    "    GBM = 'GBM'\n",
    "    XGB = 'XGBOOST'\n",
    "    LGB = 'LightGBM'\n",
    "    RF = 'RF'\n",
    "    LR = 'LR'\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        if self is BinaryClassifier.GBM:\n",
    "            import work.marvin.binary_classifier_models.bestGbdtModelProducer\n",
    "            return work.marvin.binary_classifier_models.bestGbdtModelProducer\n",
    "        elif self is BinaryClassifier.XGB:\n",
    "            import work.marvin.binary_classifier_models.bestXgboostModelProducer\n",
    "            return work.marvin.binary_classifier_models.bestXgboostModelProducer\n",
    "        elif self is BinaryClassifier.RF:\n",
    "            import work.marvin.binary_classifier_models.bestRfModelProducer\n",
    "            return work.marvin.binary_classifier_models.bestRfModelProducer\n",
    "        elif self is BinaryClassifier.LR:\n",
    "            import work.marvin.binary_classifier_models.bestLrModelProducer\n",
    "            return work.marvin.binary_classifier_models.bestLrModelProducer\n",
    "\n",
    "    def produceBestModel(self, traindf, testdf, dfm, fig_dir):\n",
    "        param_grid = self.parameterGrid(traindf, dfm)\n",
    "        return self.model.produceBestModel(traindf, testdf, dfm, param_grid, fig_dir)\n",
    "\n",
    "    def optimizeBestModel(self, traindf, testdf, dfm, fig_dir, search_alg='GP', n_calls=100):\n",
    "        configspace = self.configspace(traindf, dfm)\n",
    "        optimize_method = skopt_search(search_alg).search\n",
    "        try:\n",
    "            return self.model.optimizeBestModel(traindf, testdf, dfm, configspace,\n",
    "                                                optimize_method, fig_dir, n_calls=n_calls)\n",
    "        except Exception as e:\n",
    "            logger.error(\"optimize {} with {} error: {}\".format(self, search_alg, e))\n",
    "            raise\n",
    "\n",
    "    def parameterGrid(self, traindf, dfm):\n",
    "        if self is BinaryClassifier.LR:\n",
    "            return [{'penalty': ['l1', 'l2']}]\n",
    "        else:\n",
    "            train_array = dfm.transform(traindf)\n",
    "            train = train_array[:, :-1]\n",
    "            param_grid = self.model.parameterGridInitialization(train)\n",
    "            return [param_grid] if self is BinaryClassifier.RF else param_grid\n",
    "\n",
    "    def configspace(self, traindf, dfm):\n",
    "        if self is BinaryClassifier.LR:\n",
    "            return {'penalty': ['l1', 'l2']}\n",
    "        else:\n",
    "            train_array = dfm.transform(traindf)\n",
    "            train = train_array[:, :-1]\n",
    "            return self.model.configSpaceInitialization(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "class skopt_search(Enum):\n",
    "    gp = \"GP\"\n",
    "    rf = \"RF\"\n",
    "    gbrt = \"GBRT\"\n",
    "\n",
    "    @property\n",
    "    def method(self):\n",
    "        if self is skopt_search.gp:\n",
    "            from skopt import gp_minimize\n",
    "            return gp_minimize\n",
    "        elif self is skopt_search.rf:\n",
    "            from skopt import forest_minimize\n",
    "            return forest_minimize\n",
    "        elif self is skopt_search.gbrt:\n",
    "            from skopt import gbrt_minimize\n",
    "            return gbrt_minimize\n",
    "\n",
    "    def search(self, X_train, y_train, model_class, param_grid, loss, n_calls=100):\n",
    "        \"\"\"\n",
    "        General method for applying `skopt_method` to the data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : np.array\n",
    "            The design matrix, dimension `(n_samples, n_features)`.\n",
    "\n",
    "        y_train : list or np.array\n",
    "            The target, of dimension `n_samples`.\n",
    "\n",
    "        model_class : classifier\n",
    "            A classifier model in the mode of `sklearn`, with at least\n",
    "            `fit` and `predict` methods operating on things like\n",
    "            `X` and `y`.\n",
    "\n",
    "        param_grid : dict\n",
    "            Map from parameter names to pairs of values specifying the\n",
    "            upper and lower ends of the space from which to sample.\n",
    "            The values can also be directly specified as `skopt`\n",
    "            objects like `Categorical`.\n",
    "\n",
    "        loss : function or string\n",
    "            An appropriate loss function or string recognizable by\n",
    "            sklearn.cross_validation.cross_val_score. In sklearn, scores\n",
    "            are positive and losses are negative because they maximize,\n",
    "            but here we are minimizing so we always want smaller to mean\n",
    "            better.\n",
    "\n",
    "        n_calls : int\n",
    "            Number of evaluations to do.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of dict\n",
    "            Each has keys 'loss' and 'params', where 'params' stores the\n",
    "            values from `param_grid` for that run. The primary organizing\n",
    "            value is 'loss'.\n",
    "        Example\n",
    "        -------\n",
    "        >>> skopt_grid = {\n",
    "                'max_depth': (4, 12),\n",
    "                'learning_rate': (0.01, 0.5),\n",
    "                'n_estimators': (20, 200),\n",
    "                'objective' : Categorical(('multi:softprob',)),\n",
    "                'gamma': (0, 0.5),\n",
    "                'min_child_weight': (1, 5),\n",
    "                'subsample': (0.1, 1),\n",
    "                'colsample_bytree': (0.1, 1)}\n",
    "        >>> res = skopt_search('RF').search(X, y, XGBClassifier, skopt_grid, LOG_LOSS, n_calls=10)\n",
    "\n",
    "        To be followed by (see below):\n",
    "\n",
    "        >>> best_params, best_loss = best_results(res)\n",
    "        \"\"\"\n",
    "        logger.debug(\"---------------  skopt_search start --------------\")\n",
    "        param_keys, param_vecs = zip(*param_grid.items())\n",
    "        param_keys = list(param_keys)\n",
    "        param_vecs = list(param_vecs)\n",
    "\n",
    "        def skopt_scorer(param_vec):\n",
    "            params = dict(zip(param_keys, param_vec))\n",
    "            logger.debug(params)\n",
    "            err = cross_validated_scorer(\n",
    "                X_train, y_train, model_class, params, loss)\n",
    "            return err\n",
    "\n",
    "        try:\n",
    "            outcome = self.method(skopt_scorer, list(param_vecs), n_calls=n_calls)\n",
    "            results = []\n",
    "            for err, param_vec in zip(outcome.func_vals, outcome.x_iters):\n",
    "                params = dict(zip(param_keys, param_vec))\n",
    "                results.append({'loss': err, 'params': params})\n",
    "            logger.debug(\"---------------  skopt_search end --------------\")\n",
    "        except Exception as e:\n",
    "            logger.error(e)\n",
    "            raise\n",
    "        return results\n",
    "\n",
    "        def skopt_scorer(param_vec):\n",
    "            params = dict(zip(param_keys, param_vec))\n",
    "\n",
    "            err = cross_validated_scorer(\n",
    "                X_train, y_train, model_class, params, loss)\n",
    "            return err\n",
    "\n",
    "        outcome = self.method(skopt_scorer, list(param_vecs), n_calls=n_calls)\n",
    "        results = []\n",
    "        for err, param_vec in zip(outcome.func_vals, outcome.x_iters):\n",
    "            params = dict(zip(param_keys, param_vec))\n",
    "            results.append({'loss': err, 'params': params})\n",
    "        logger.debug(\"---------------  skopt_search end --------------\")\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Loss(Enum):\n",
    "    LOG_LOSS = 'neg_log_loss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def prepareDataforTraining(transformed, datamapper, train_size=0.75):\n",
    "    traindf, testdf = cross_validation.train_test_split(transformed, train_size=train_size)\n",
    "    return traindf, testdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def modelfit(alg, datamapper, train, labels_train, test, labels_test,\n",
    "             fig_path=None, cv_folds=5, most_importance_n=20):\n",
    "    alg.fit(train, labels_train)\n",
    "    train_predictions = alg.predict(train)\n",
    "    train_predprob = alg.predict_proba(train)[:, 1]\n",
    "\n",
    "    cv_score = cross_validation.cross_val_score(alg, train, labels_train,\n",
    "                                                cv=cv_folds, n_jobs=cv_folds, scoring='roc_auc')\n",
    "\n",
    "    feature_list = [mapper.data_ for (name, mapper) in datamapper.features if mapper]\n",
    "    feature_indices = [feature for sublist in feature_list for feature in sublist]\n",
    "    if hasattr(alg, 'feature_importances_'):\n",
    "        feature_importances = pd.DataFrame([alg.feature_importances_], columns=feature_indices)\n",
    "    elif hasattr(alg, 'coef_'):\n",
    "        feature_importances = pd.DataFrame(alg.coef_, columns=feature_indices)\n",
    "    else:\n",
    "        raise Exception('unrecognized algorithm')\n",
    "    sorted_abs_importances = feature_importances.ix[0, :].abs().sort_values(ascending=False)\n",
    "    sorted_feature_importances = sorted_abs_importances.index[:most_importance_n]\n",
    "    feature_importances = feature_importances[sorted_feature_importances]\n",
    "    logger.debug(\"plot feature_importances\")\n",
    "    # Plot barchart\n",
    "    try:\n",
    "        sns.plt.clf()\n",
    "        sns.plt.figure(figsize=(8, 6))\n",
    "        sns.barplot(x=[col.decode(\"utf-8\") for col in feature_importances.columns],\n",
    "                    y=np.array(feature_importances)[0, :],\n",
    "                    label='small')\n",
    "        sns.plt.title('Feature Importances')\n",
    "        sns.plt.xlabel('Feature')\n",
    "        sns.plt.xticks(rotation=90)\n",
    "        sns.plt.ylabel('Feature Importance Score')\n",
    "        sns.plt.tight_layout()\n",
    "        if fig_path is not None:\n",
    "                sns.plt.savefig(fig_path)\n",
    "    except Exception as e:\n",
    "        raise Exception(\"save fig {} error: {}\".format(fig_path, e))\n",
    "    return alg, train_predictions, train_predprob, cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def run_experiments(\n",
    "        experimental_run,\n",
    "        trainX,\n",
    "        trainY,\n",
    "        model_class,\n",
    "        loss=Loss.LOG_LOSS,\n",
    "        test_metric=roc_auc_score,\n",
    "        random_state=None,\n",
    "        dataset_name=None):\n",
    "    \"\"\"\n",
    "    Basic experimental framework.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    experimental_run : list of tuples\n",
    "        These tuples should have exactly three members: the first one\n",
    "        of `grid_search`, `randomized_search`, `hyperopt_search`,\n",
    "        `skopt_gp_minimize`, `skopt_forest_minimize`, or\n",
    "        `skopt_forest_gbrt`, the second an appropriate `param_grid`\n",
    "        dict for that function, and the third a dict specifying\n",
    "        keyword arguments to the search function.\n",
    "\n",
    "    dataset : (np.array, iterable)\n",
    "        A dataset (X, y) where `X` has dimension\n",
    "        `(n_samples, n_features)` and `y` has\n",
    "         dimension `n_samples`.\n",
    "\n",
    "    model_class : classifier\n",
    "        A classifier model in the mode of `sklearn`, with at least\n",
    "        `fit` and `predict` methods operating on things like\n",
    "        `X` and `y`.\n",
    "\n",
    "    loss : function or string\n",
    "        An appropriate loss function or string recognizable by\n",
    "        `sklearn.cross_validation.cross_val_score`. In `sklearn`, scores\n",
    "        are positive and losses are negative because they maximize,\n",
    "        but here we are minimizing so we always want smaller to mean\n",
    "        better.\n",
    "\n",
    "    test_metric : function\n",
    "        An `sklearn.metrics` function.\n",
    "\n",
    "    random_state : int\n",
    "\n",
    "    dataset_name : str or None\n",
    "        Informal name to give the dataset. Purely for\n",
    "        book-keeping.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of dict\n",
    "       Each dict is a results dictionary of the sort returned\n",
    "       by `assess`.\n",
    "    \"\"\"\n",
    "    X = trainX\n",
    "    y = trainY\n",
    "\n",
    "    skf = get_cross_validation_indices(\n",
    "        X, y, n_folds=2, random_state=random_state)\n",
    "\n",
    "    all_results = []\n",
    "    # This loop can easily be parallelized, but doing so can\n",
    "    # be tricky on some systems, since `cross_val_score`\n",
    "    # calls `joblib` even if `n_jobs=1`, resulting in\n",
    "    # nested parallel jobs even if there is no actual\n",
    "    # parallelization elsewhere in the experimental run.\n",
    "    for search_func, param_grid, kwargs in experimental_run:\n",
    "        logger.info(search_func.__name__)\n",
    "        all_results.append(\n",
    "            assess(\n",
    "                X, y,\n",
    "                search_func=search_func,\n",
    "                model_class=model_class,\n",
    "                param_grid=param_grid,\n",
    "                xval_indices=skf,\n",
    "                loss=loss.value,\n",
    "                test_metric=test_metric,\n",
    "                dataset_name=dataset_name,\n",
    "                search_func_args=kwargs))\n",
    "        logger.info(\"--------------- assess end --------------\")\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def assess(\n",
    "        X, y,\n",
    "        search_func,\n",
    "        model_class,\n",
    "        param_grid,\n",
    "        xval_indices,\n",
    "        loss,\n",
    "        test_metric=roc_auc_score,\n",
    "        dataset_name=None,\n",
    "        search_func_args={}):\n",
    "    \"\"\"\n",
    "    The core of the experimental framework. This runs cross-validation\n",
    "    and, for the inner loop, does cross-validation to find the optimal\n",
    "    hyperparameters according to `search_func`. These optimal\n",
    "    parameters are then used for an assessment in the outer\n",
    "    cross-validation run.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "        The design matrix, dimension `(n_samples, n_features)`.\n",
    "\n",
    "    y : list or np.array\n",
    "        The target, of dimension `n_samples`.\n",
    "\n",
    "    search_func : function\n",
    "        The search function to use. Can be `grid_search`,\n",
    "        `randomized_search`, `hyperopt_search`, `skopt_gp_minimize`,\n",
    "        `skopt_forest_minimize`, or `skopt_forest_gbrt`, all\n",
    "        defined in this module. This choice has to be compatible with\n",
    "        `param_grid`, in the sense that `grid_search` and\n",
    "        `randomized_search` require a dict from strings to lists of\n",
    "        values, `hyperopt_search` requires a dict from strings to\n",
    "        hyperopt sampling functions, and the `skopt` functions\n",
    "        require dicts from strings to (upper, lower) pairs of\n",
    "        special `skopt` functions.\n",
    "\n",
    "    model_class : classifier\n",
    "        A classifier model in the mode of `sklearn`, with at least\n",
    "        `fit` and `predict` methods operating on things like\n",
    "        `X` and `y`.\n",
    "\n",
    "    param_grid : dict\n",
    "        Map from parameter names to appropriate specifications of\n",
    "        appropriate values for that parameter. This is not the\n",
    "        expanded grid, but rather the simple map that can be expanded\n",
    "        by `expand_grid` below (though not all methods call for that).\n",
    "        This has to be compatible with  `search_func`, and all the\n",
    "        values must be suitable arguments to `model_class` instances.\n",
    "\n",
    "    loss : function or string\n",
    "        An appropriate loss function or string recognizable by\n",
    "        `sklearn.cross_validation.cross_val_score`. In `sklearn`, scores\n",
    "        are positive and losses are negative because they maximize,\n",
    "        but here we are minimizing so we always want smaller to mean\n",
    "        better.\n",
    "\n",
    "    test_metric : function\n",
    "        An `sklearn.metrics` function.\n",
    "\n",
    "    xval_indices : list\n",
    "        List of train and test indices into `X` and `y`. This defines\n",
    "        the cross-validation. This is done outside of this method to\n",
    "        allow for identical splits across different experiments.\n",
    "\n",
    "    dataset_name : str or None\n",
    "        Name for the dataset being analyzed. For book-keeping and\n",
    "        display only.\n",
    "\n",
    "    search_func_args : dict\n",
    "        Keyword arguments to feed to `search_func`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Accumulated information about the experiment:\n",
    "\n",
    "        {'Test accuracy': list of float,\n",
    "         'Cross-validation time':list of float,\n",
    "         'Parameters sampled': list of int,\n",
    "         'Method': search_func.__name__,\n",
    "         'Model': model_class.__name__,\n",
    "         'Dataset': dataset_name,\n",
    "         'Best parameters': list of dict,\n",
    "         'Mean test accuracy': float,\n",
    "         'Mean cross-validation time': float,\n",
    "         'Mean parameters sampled': float}   \n",
    "    \"\"\"\n",
    "    logger.info(\"assess: {}\".format(test_metric))\n",
    "    data = {'Test accuracy': [],\n",
    "            'Cross-validation time': [],\n",
    "            'Parameters sampled': [],\n",
    "            'Best parameters': [],\n",
    "            'Method': search_func.__name__,\n",
    "            'Model': model_class.__name__,\n",
    "            'Dataset': dataset_name,\n",
    "            'Best parameters': []}\n",
    "    for cv_index, (train_index, test_index) in enumerate(xval_indices, start=1):\n",
    "        logger.info(\"\\t{}\".format(cv_index))\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        start = time()\n",
    "        results = search_func(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            model_class,\n",
    "            param_grid,\n",
    "            loss,\n",
    "            **search_func_args)\n",
    "        data['Cross-validation time'].append(time() - start)\n",
    "        data['Parameters sampled'].append(len(results))\n",
    "        best_params = sorted(results, key=itemgetter('loss'), reverse=False)\n",
    "        best_params = best_params[0]['params']\n",
    "        data['Best parameters'].append(best_params)\n",
    "        bestmod = model_class(**best_params)\n",
    "        bestmod.fit(X_train, y_train)\n",
    "        predictions = bestmod.predict(X_test)\n",
    "        data['Test accuracy'].append(test_metric(y_test, predictions))\n",
    "        data['Mean test accuracy'] = np.mean(data['Test accuracy'])\n",
    "        data['Mean cross-validation time'] = np.mean(data['Cross-validation time'])\n",
    "        data['Mean parameters sampled'] = np.mean(data['Parameters sampled'])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def get_cross_validation_indices(X, y, n_folds=5, random_state=None):\n",
    "    \"\"\"\n",
    "    Use `StratifiedKFold` to create an `n_folds` cross-validator for\n",
    "    the dataset defined by `X` and y`. Only `y` is used, but both are\n",
    "    given for an intuitive interface; `X` could just as easily be used.\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_folds, random_state=random_state)\n",
    "    return skf.split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validated_scorer(\n",
    "        X_train, y_train, model_class, params, loss, kfolds=5):\n",
    "    \"\"\"\n",
    "    The scoring function used through this module, by all search\n",
    "    functions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : np.array\n",
    "        The design matrix, dimension `(n_samples, n_features)`.\n",
    "\n",
    "    y_train : list or np.array\n",
    "        The target, of dimension `n_samples`.\n",
    "\n",
    "    model_class : classifier\n",
    "        A classifier model in the mode of `sklearn`, with at least\n",
    "        `fit` and `predict` methods operating on things like\n",
    "        `X` and `y`.\n",
    "\n",
    "    params : dict\n",
    "        Map from parameter names to single appropriate values\n",
    "        for that parameter. This will be used to build a model\n",
    "        from `model_class`.\n",
    "\n",
    "    loss : function or string\n",
    "        An appropriate loss function or string recognizable by\n",
    "        sklearn.cross_validation.cross_val_score. In sklearn, scores\n",
    "        are positive and losses are negative because they maximize,\n",
    "        but here we are minimizing so we always want smaller to mean\n",
    "        better.\n",
    "\n",
    "    kfolds : int\n",
    "        Number of cross-validation runs to do.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "       Average loss over the `kfolds` runs.\n",
    "    \"\"\"\n",
    "    mod = model_class(**params)\n",
    "    cv_score = -1 * cross_validation.cross_val_score(\n",
    "        mod,\n",
    "        X_train,\n",
    "        y=y_train,\n",
    "        scoring=loss,\n",
    "        cv=kfolds,\n",
    "        n_jobs=1).mean()\n",
    "    return cv_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
