{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "import logging\n",
    "import work.marvin.binary_classifier_models.modelfit as modelfit\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def bestModelProducer(data, target, datamapper, figpath):\n",
    "    \"\"\"\n",
    "    auto xgboost model generation, 3 steps:\n",
    "    1. estimate optimal model parameters space for gridsearch,\n",
    "       depends on sample size and feature size\n",
    "    2. run gridsearch to find best parameter set\n",
    "    3. train the best GBDT model using the best parameter set\n",
    "    \"\"\"\n",
    "    traindf, testdf = modelfit.prepareDataforTraining(data, datamapper)\n",
    "    train_array = datamapper.transform(traindf)\n",
    "    train = train_array[:, :-1]\n",
    "\n",
    "    # estimate optimal parameters grid space\n",
    "    param_grid1, param_grid2, param_grid3, param_grid4 = parameterGridInitialization(train)\n",
    "    alg, accuracy, auc, cv_score = produceBestXgboostModel(traindf, testdf, datamapper,\n",
    "                                                           param_grid1, param_grid2,\n",
    "                                                           param_grid3, param_grid4,\n",
    "                                                           figpath)\n",
    "    return alg, traindf, testdf, accuracy, auc, cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def parameterGridInitialization(trainX):\n",
    "    feature_size = trainX.shape[1] - 1\n",
    "    train_size = trainX.shape[0]\n",
    "\n",
    "    n_estimators = [1000]\n",
    "\n",
    "    subsample_spc = [0.6, 0.7, 0.8, 0.9]\n",
    "    colsample_bytree_spc = [0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "    gamma_spc = [i / 10.0 for i in range(0, 5)]\n",
    "    reg_alpha_spc = [0, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "    learning_rate_spc = [0.01, 0.05, 0.1]\n",
    "\n",
    "    max_depth_spc = max_depth_space(feature_size)\n",
    "    min_child_weight_spc = min_child_weight_space(train_size)\n",
    "\n",
    "    # set learning_rate, run to get optiomal n_estimators\n",
    "    param_grid1 = {'n_estimators': n_estimators}\n",
    "\n",
    "    # most important parameters\n",
    "    param_grid2 = {'max_depth': max_depth_spc, 'min_child_weight': min_child_weight_spc,\n",
    "                   'subsample': subsample_spc, 'colsample_bytree': colsample_bytree_spc}\n",
    "\n",
    "    # regularization parameters\n",
    "    param_grid3 = {'gamma': gamma_spc, 'reg_alpha': reg_alpha_spc}\n",
    "\n",
    "    # learning_rate parameters\n",
    "    param_grid4 = {'learning_rate': learning_rate_spc}\n",
    "\n",
    "    return param_grid1, param_grid2, param_grid3, param_grid4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def produceBestXgboostModel(traindf, testdf, datamapper,\n",
    "                            param_grid1, param_grid2, param_grid3, param_grid4,\n",
    "                            fig_path=None, seed=27):\n",
    "    # datamapper transform\n",
    "    train_array = datamapper.transform(traindf)\n",
    "    train = train_array[:, :-1]            # 默认label为最后一列\n",
    "    labels_train = train_array[:, -1]      # 默认label为最后一列\n",
    "    test_array = datamapper.transform(testdf)\n",
    "    test = test_array[:, :-1]\n",
    "    labels_test = test_array[:, -1]\n",
    "\n",
    "    # running grid search to get the best parameter set  \n",
    "    best_subsample, best_estimators, best_learning_rate, best_max_depth, best_min_child_weight, best_colsample_bytree, best_gamma, best_reg_alpha = xgboostGridSearch(train, labels_train, param_grid1, param_grid2, param_grid3, param_grid4)\n",
    "\n",
    "    # train a gbm using the best parameter set\n",
    "    xgboost_best = XGBClassifier(n_estimators=best_estimators, learning_rate=best_learning_rate,\n",
    "                                 max_depth=best_max_depth, min_child_weight=best_min_child_weight,\n",
    "                                 subsample=best_subsample, colsample_bytree=best_colsample_bytree,\n",
    "                                 gamma=best_gamma, reg_alpha=best_reg_alpha,\n",
    "                                 objective='binary:logistic', nthread=-1,\n",
    "                                 scale_pos_weight=1, seed=seed)\n",
    "\n",
    "    alg, train_predictions, train_predprob, cv_score = modelfit.modelfit(xgboost_best, datamapper,\n",
    "                                                                         train, labels_train,\n",
    "                                                                         test, labels_test,\n",
    "                                                                         fig_path)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(labels_train, train_predictions)\n",
    "    auc = metrics.roc_auc_score(labels_train, train_predprob)\n",
    "    cv_score = [np.mean(cv_score), np.std(cv_score), np.min(cv_score), np.max(cv_score)]\n",
    "\n",
    "    return alg, accuracy, auc, cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def max_depth_space(feature_size):\n",
    "    if feature_size > 1000:\n",
    "        max_depth = range(5, 14, 2)\n",
    "    else:\n",
    "        max_depth = range(3, 10, 2)\n",
    "    return max_depth\n",
    "\n",
    "\n",
    "# <api>\n",
    "def min_child_weight_space(train_size):\n",
    "    if train_size > 10000:\n",
    "        min_child_weight = range(3, 8, 1)\n",
    "    else:\n",
    "        min_child_weight = range(1, 6, 1)\n",
    "    return min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "# train xgBoost to get best n_estimators\n",
    "def xgBoostTrainBestn_estimators(alg, dtrain, dtest,\n",
    "                                 useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    if not useTrainCV:\n",
    "        return\n",
    "    xgb_param = alg.get_xgb_params()\n",
    "    xgtrain = xgb.DMatrix(dtrain, label=dtest)\n",
    "    cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'],\n",
    "                      nfold=cv_folds,\n",
    "                      metrics=['auc'], early_stopping_rounds=early_stopping_rounds)\n",
    "    return cvresult.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def xgboostGridSearch(train, labels_train,\n",
    "                      param_grid1, param_grid2, param_grid3, param_grid4, seed=27):\n",
    "\n",
    "    estimator = XGBClassifier(max_depth=3, min_child_weight=1, gamma=0, subsample=0.8, learning_rate=0.1,\n",
    "                              n_estimators=param_grid1['n_estimators'][0], colsample_bytree=0.8,\n",
    "                              objective='binary:logistic',\n",
    "                              nthread=-1, scale_pos_weight=1, seed=27)\n",
    "\n",
    "    best_estimators = xgBoostTrainBestn_estimators(estimator, train, labels_train)\n",
    "\n",
    "    gsearch1 = GridSearchCV(estimator=XGBClassifier(n_estimators=best_estimators,\n",
    "                                                    learning_rate=0.1, gamma=0,\n",
    "                                                    objective='binary:logistic', nthread=-1, scale_pos_weight=1,\n",
    "                                                    seed=27),\n",
    "                            param_grid=param_grid2, scoring='roc_auc',\n",
    "                            n_jobs=1, iid=False, cv=5)\n",
    "    gsearch1.fit(train, labels_train)\n",
    "\n",
    "    best_parameters = gsearch1.best_estimator_.get_params()\n",
    "    best_max_depth = best_parameters[\"max_depth\"]\n",
    "    best_min_child_weight = best_parameters['min_child_weight']\n",
    "    best_subsample = best_parameters['subsample']\n",
    "    best_colsample_bytree = best_parameters['colsample_bytree']\n",
    "\n",
    "    gsearch2 = GridSearchCV(estimator=XGBClassifier(n_estimators=best_estimators, learning_rate=0.1,\n",
    "                                                    max_depth=best_max_depth, min_child_weight=best_min_child_weight,\n",
    "                                                    subsample=best_subsample, colsample_bytree=best_colsample_bytree,\n",
    "                                                    objective='binary:logistic', nthread=-1, scale_pos_weight=1,\n",
    "                                                    seed=27),\n",
    "                            param_grid=param_grid3, scoring='roc_auc',\n",
    "                            n_jobs=1, iid=False, cv=5)\n",
    "    gsearch2.fit(train, labels_train)\n",
    "\n",
    "    best_parameters = gsearch2.best_estimator_.get_params()\n",
    "    best_gamma = best_parameters[\"gamma\"]\n",
    "    best_reg_alpha = best_parameters[\"reg_alpha\"]\n",
    "\n",
    "    gsearch3 = GridSearchCV(estimator=XGBClassifier(n_estimators=best_estimators,\n",
    "                                                    max_depth=best_max_depth, min_child_weight=best_min_child_weight,\n",
    "                                                    subsample=best_subsample, colsample_bytree=best_colsample_bytree,\n",
    "                                                    gamma=best_gamma, reg_alpha=best_reg_alpha,\n",
    "                                                    objective='binary:logistic', nthread=-1, scale_pos_weight=1,\n",
    "                                                    seed=seed),\n",
    "                            param_grid=param_grid4, scoring='roc_auc',\n",
    "                            n_jobs=1, iid=False, cv=5)\n",
    "    gsearch3.fit(train, labels_train)\n",
    "\n",
    "    best_parameters = gsearch3.best_estimator_.get_params()\n",
    "    best_learning_rate = best_parameters[\"learning_rate\"]\n",
    "\n",
    "    estimator = XGBClassifier(n_estimators=param_grid1['n_estimators'][0]*2, learning_rate=best_learning_rate,\n",
    "                              max_depth=best_max_depth, min_child_weight=best_min_child_weight,\n",
    "                              subsample=best_subsample, colsample_bytree=best_colsample_bytree,\n",
    "                              gamma=best_gamma, reg_alpha=best_reg_alpha, objective='binary:logistic', nthread=-1,\n",
    "                              scale_pos_weight=1, seed=27)\n",
    "\n",
    "    best_estimators = xgBoostTrainBestn_estimators(estimator, train, labels_train)\n",
    "\n",
    "    return best_subsample, best_estimators, best_learning_rate, best_max_depth,\\\n",
    "            best_min_child_weight, best_colsample_bytree, best_gamma, best_reg_alpha"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
