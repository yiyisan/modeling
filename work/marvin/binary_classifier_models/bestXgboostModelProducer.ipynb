{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import seaborn as sns\n",
    "\n",
    "import logging\n",
    "import work.marvin.binary_classifier_models.modelfit as modelfit\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def bestModelProducer(data, target, datamapper, figpath):\n",
    "    \"\"\"\n",
    "    # auto xgboost model generation, 3 steps:\n",
    "    1. estimate optimal model parameters space for gridsearch, depends on sample size and feature size\n",
    "    2. run gridsearch to find best parameter set\n",
    "    3. train the best GBDT model using the best parameter set\n",
    "    \"\"\"\n",
    "    traindf, testdf = modelfit.prepareDataforTraining(data, datamapper)\n",
    "    train_array = datamapper.transform(traindf)\n",
    "    train = train_array[:, :-1] \n",
    "    \n",
    "    # estimate optimal parameters grid space\n",
    "    param_grid1, param_grid2, param_grid3, param_grid4 = parameterGridInitialization(train)\n",
    "    alg, accuracy, auc, cv_score = produceBestXgboostModel(traindf, testdf, datamapper,\n",
    "                                                           param_grid1, param_grid2, param_grid3, param_grid4,\n",
    "                                                           figpath)\n",
    "    return alg, traindf, testdf, accuracy, auc, cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def parameterGridInitialization(trainX):\n",
    "    feature_size = trainX.shape[1] - 1  \n",
    "    train_size = trainX.shape[0]\n",
    "    \n",
    "    n_estimators = [1000]\n",
    "    \n",
    "    subsample_spc = [0.6, 0.7, 0.8, 0.9]\n",
    "    colsample_bytree_spc = [0.6, 0.7, 0.8, 0.9]\n",
    "    \n",
    "    gamma_spc = [i / 10.0 for i in range(0, 5)]   \n",
    "    reg_alpha_spc = [0, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "    \n",
    "    learning_rate_spc = [0.01, 0.05, 0.1]\n",
    "    \n",
    "    max_depth_spc = max_depth_space(feature_size)\n",
    "    min_child_weight_spc = min_child_weight_space(train_size)\n",
    "    \n",
    "    # set learning_rate, run to get optiomal n_estimators\n",
    "    param_grid1 = {'n_estimators': n_estimators}\n",
    "    \n",
    "    # most important parameters    \n",
    "    param_grid2 = {'max_depth': max_depth_spc, 'min_child_weight': min_child_weight_spc, \n",
    "                   'subsample': subsample_spc, 'colsample_bytree': colsample_bytree_spc}\n",
    "    \n",
    "    # regularization parameters\n",
    "    param_grid3 = {'gamma': gamma_spc, 'reg_alpha': reg_alpha_spc}\n",
    "    \n",
    "    # learning_rate parameters\n",
    "    param_grid4 = {'learning_rate': learning_rate_spc}\n",
    "    \n",
    "    return param_grid1, param_grid2, param_grid3, param_grid4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def produceBestXgboostModel(traindf, testdf, datamapper, param_grid1, param_grid2, param_grid3, param_grid4, fig_path=None):\n",
    "\n",
    "    # datamapper transform\n",
    "    train_array = datamapper.transform(traindf)\n",
    "    train = train_array[:, :-1]            # 默认label为最后一列\n",
    "    labels_train = train_array[:, -1]      # 默认label为最后一列\n",
    "    test_array = datamapper.transform(testdf)\n",
    "    test = test_array[:, :-1]\n",
    "    labels_test = test_array[:, -1]\n",
    " \n",
    "    # running grid search to get the best parameter set  \n",
    "    best_subsample,best_estimators,best_learning_rate,best_max_depth,best_min_child_weight,best_colsample_bytree,best_gamma,best_reg_alpha = xgboostGridSearch(train, labels_train, param_grid1, param_grid2, param_grid3, param_grid4)\n",
    "    \n",
    "    # train a gbm using the best parameter set\n",
    "    xgboost_best = XGBClassifier(n_estimators=best_estimators, learning_rate=best_learning_rate,\n",
    "                                 max_depth=best_max_depth, min_child_weight=best_min_child_weight,\n",
    "                                 subsample=best_subsample, colsample_bytree=best_colsample_bytree, \n",
    "                                 gamma=best_gamma, reg_alpha=best_reg_alpha, objective='binary:logistic', nthread=4,\n",
    "                                 scale_pos_weight=1, seed=27)\n",
    "\n",
    "    alg, train_predictions, train_predprob, cv_score = modelfit.modelfit(xgboost_best, datamapper, train, labels_train, test, labels_test, fig_path)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(labels_train, train_predictions)\n",
    "    auc = metrics.roc_auc_score(labels_train, train_predprob)\n",
    "    cv_score = [np.mean(cv_score), np.std(cv_score), np.min(cv_score), np.max(cv_score)]\n",
    "\n",
    "    return alg, accuracy, auc, cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def _modelfit(alg, datamapper, train, labels_train, test, labels_test, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(train, label=labels_train)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "                          metrics=['auc'], early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(train, labels_train, eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(train)\n",
    "    dtrain_predprob = alg.predict_proba(train)[:,1]\n",
    "\n",
    "    #Print Feature Importance:\n",
    "    trans = [[\"{}={}\".format(name, str(cls)) for cls in mapper.classes_.tolist()]\n",
    "             if isinstance(mapper, LabelBinarizer) else [name] for (name, mapper) in datamapper.features]\n",
    "    feature_indices = [i for sublist in trans for i in sublist][1:]\n",
    "    xg_feature_importances = pd.DataFrame([alg.feature_importances_], columns = feature_indices)\n",
    "    sorted_feature_importances = xg_feature_importances.ix[0, :].sort_values(ascending=False).index[:20] # 这里的20表示按重要性顺序取前20个\n",
    "    feature_importances = xg_feature_importances[sorted_feature_importances]\n",
    "        \n",
    "    sns.barplot(x=feature_importances.columns, y=np.array(feature_importances)[0,:])\n",
    "    sns.plt.title('Feature Importances')\n",
    "    sns.plt.xlabel('Feature')\n",
    "    sns.plt.xticks(rotation=90)\n",
    "    sns.plt.ylabel('Feature Importance Score')\n",
    "    sns.plt.savefig('featureimportance.png')\n",
    "        \n",
    "    #Print model report:\n",
    "    logger.info(\"Model Report\")\n",
    "    logger.info(\"Accuracy : %.4g\" % metrics.accuracy_score(labels_train, dtrain_predictions))\n",
    "    logger.info(\"AUC Score (Train): %f\" % metrics.roc_auc_score(labels_train, dtrain_predprob))\n",
    "\n",
    "    return alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def max_depth_space(feature_size):\n",
    "    if feature_size > 1000 :\n",
    "        max_depth = range(5,14,2)\n",
    "    else :\n",
    "        max_depth = range(3,10,2)      \n",
    "    return max_depth\n",
    "\n",
    "# <api>\n",
    "def min_child_weight_space(train_size):\n",
    "    if train_size > 10000 :\n",
    "        min_child_weight = range(3,8,1)\n",
    "    else :\n",
    "        min_child_weight = range(1,6,1)      \n",
    "    return min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "# train xgBoost to get best n_estimators\n",
    "def xgBoostTrainBestn_estimators(alg, dtrain, dtest, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain, label=dtest)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "                          metrics=['auc'], early_stopping_rounds=early_stopping_rounds)\n",
    "        return cvresult.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def xgboostGridSearch(train, labels_train, param_grid1, param_grid2, param_grid3, param_grid4):\n",
    "    \n",
    "    estimator = XGBClassifier(max_depth=3, min_child_weight=1, gamma=0, subsample=0.8, learning_rate=0.1,\n",
    "                              n_estimators=param_grid1['n_estimators'][0], colsample_bytree=0.8, objective='binary:logistic', \n",
    "                              nthread=4, scale_pos_weight=1, seed=27)\n",
    "    \n",
    "    best_estimators = xgBoostTrainBestn_estimators(estimator, train, labels_train)\n",
    "    \n",
    "    gsearch1 = GridSearchCV(estimator = XGBClassifier(n_estimators=best_estimators, learning_rate=0.1,\n",
    "                                                      gamma=0, objective= 'binary:logistic', nthread=4, scale_pos_weight=1, \n",
    "                                                      seed=27), param_grid = param_grid2, scoring='roc_auc',n_jobs=4,\n",
    "                                                      iid=False, cv=5)  \n",
    "    gsearch1.fit(train, labels_train)\n",
    "    \n",
    "    best_parameters = gsearch1.best_estimator_.get_params()\n",
    "    best_max_depth = best_parameters[\"max_depth\"]   \n",
    "    best_min_child_weight = best_parameters['min_child_weight']\n",
    "    best_subsample = best_parameters['subsample']\n",
    "    best_colsample_bytree = best_parameters['colsample_bytree']\n",
    "    \n",
    "    gsearch2 = GridSearchCV(estimator = XGBClassifier(n_estimators=best_estimators, learning_rate=0.1,\n",
    "                                                      max_depth=best_max_depth, min_child_weight=best_min_child_weight,\n",
    "                                                      subsample=best_subsample, colsample_bytree=best_colsample_bytree,\n",
    "                                                      objective= 'binary:logistic', nthread=4, scale_pos_weight=1, \n",
    "                                                      seed=27), param_grid = param_grid3, scoring='roc_auc', n_jobs=4,\n",
    "                                                      iid=False, cv=5)  \n",
    "    gsearch2.fit(train, labels_train)  \n",
    "    \n",
    "    best_parameters = gsearch2.best_estimator_.get_params()    \n",
    "    best_gamma = best_parameters[\"gamma\"]\n",
    "    best_reg_alpha = best_parameters[\"reg_alpha\"]\n",
    "    \n",
    "    gsearch3 = GridSearchCV(estimator = XGBClassifier(n_estimators=best_estimators, \n",
    "                                                      max_depth=best_max_depth, min_child_weight=best_min_child_weight,\n",
    "                                                      subsample=best_subsample, colsample_bytree=best_colsample_bytree,\n",
    "                                                      gamma=best_gamma, reg_alpha=best_reg_alpha,\n",
    "                                                      objective= 'binary:logistic', nthread=4, scale_pos_weight=1, \n",
    "                                                      seed=27), param_grid = param_grid4, scoring='roc_auc',n_jobs=4,\n",
    "                                                      iid=False, cv=5)  \n",
    "    gsearch3.fit(train, labels_train)  \n",
    "    \n",
    "    best_parameters = gsearch3.best_estimator_.get_params()    \n",
    "    best_learning_rate = best_parameters[\"learning_rate\"]\n",
    "    \n",
    "    estimator = XGBClassifier(n_estimators=param_grid1['n_estimators'][0]*2, learning_rate=best_learning_rate,\n",
    "                              max_depth=best_max_depth, min_child_weight=best_min_child_weight,\n",
    "                              subsample=best_subsample, colsample_bytree=best_colsample_bytree, \n",
    "                              gamma=best_gamma, reg_alpha=best_reg_alpha, objective='binary:logistic', nthread=4, \n",
    "                              scale_pos_weight=1, seed=27)\n",
    "    \n",
    "    best_estimators = xgBoostTrainBestn_estimators(estimator, train, labels_train)\n",
    "    \n",
    "    return best_subsample, best_estimators, best_learning_rate, best_max_depth, best_min_child_weight, best_colsample_bytree, best_gamma, best_reg_alpha"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
