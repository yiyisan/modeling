{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "from skopt.space import Categorical\n",
    "import work.marvin.binary_classifier_models.modelfit as modelfit\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def bestModelProducer(data, target, datamapper, figpath=None):\n",
    "    \"\"\"\n",
    "    auto lightgbm model generation, 3 steps:\n",
    "    1. estimate optimal model parameters space for gridsearch,\n",
    "       depends on sample size and feature size\n",
    "    2. run gridsearch to find best parameter set\n",
    "    3. train the best GBDT model using the best parameter set\n",
    "    \"\"\"\n",
    "\n",
    "    traindf, testdf = modelfit.prepareDataforTraining(data, datamapper)\n",
    "    datamapper.fit_transform(traindf[traindf.columns.difference([target])])\n",
    "    # estimate optimal parameters grid space\n",
    "    configspace = parameterGridInitialization(datamapper.shape)\n",
    "    lightgbm_best = produceBestLightgbmModel(traindf, datamapper, target, configspace, figpath)\n",
    "    return lightgbm_best, traindf, testdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def parameterGridInitialization(trainX):\n",
    "    feature_size = trainX.shape[1] - 1\n",
    "    train_size = trainX.shape[0]\n",
    "\n",
    "    n_estimators = [1000]\n",
    "\n",
    "    subsample_spc = [0.6, 0.7, 0.8, 0.9]\n",
    "    colsample_bytree_spc = [0.6, 0.7, 0.8, 0.9]\n",
    "    reg_alpha_spc = [0, 0.01, 0.1, 1, 10]\n",
    "    reg_lambda_spc = [0.1, 0.3, 0.5]\n",
    "    learning_rate_spc = [0.01, 0.05, 0.1]\n",
    "\n",
    "    max_depth_spc = max_depth_space(feature_size)\n",
    "    min_child_weight_spc = min_child_weight_space(train_size)\n",
    "\n",
    "    # set learning_rate, run to get optiomal n_estimators\n",
    "    param_grid1 = {'n_estimators': n_estimators}\n",
    "\n",
    "    # most important parameters\n",
    "    param_grid2 = {'max_depth': max_depth_spc,\n",
    "                   'min_child_weight': min_child_weight_spc,\n",
    "                   'subsample': subsample_spc,\n",
    "                   'colsample_bytree': colsample_bytree_spc}\n",
    "\n",
    "    # regularization parameters\n",
    "    param_grid3 = {'reg_alpha': reg_alpha_spc, 'reg_lambda': reg_lambda_spc}\n",
    "\n",
    "    # learning_rate parameters\n",
    "    param_grid4 = {'learning_rate': learning_rate_spc}\n",
    "\n",
    "    return param_grid1, param_grid2, param_grid3, param_grid4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def configSpaceInitialization(trainX):\n",
    "    \"\"\"\n",
    "       sample configSpace\n",
    "    \"\"\"\n",
    "    if trainX.shape[1] >= 10:\n",
    "        skopt_grid = {'max_depth': (3, 9),\n",
    "                      'learning_rate': (0.01, 0.1),\n",
    "                      'n_estimators': (50, 800),\n",
    "                      'boosting_type': Categorical(('gbdt',)),\n",
    "                      'xgboost_dart_mode': Categorical((True,)),\n",
    "                      'min_child_weight': (1, 5),\n",
    "                      'subsample_freq': (1, 5),\n",
    "                      'colsample_bytree': (0.2, 0.9),\n",
    "                      'reg_alpha': (1, 5),\n",
    "                      'reg_lambda': (0.1, 0.5),\n",
    "                      'drop_rate': (0.01, 0.5),\n",
    "                      'scale_pos_weight': (1, 5),\n",
    "                      'num_leaves' : (20, 40)}\n",
    "    else:\n",
    "        skopt_grid = {'max_depth': (2, 3),\n",
    "                      'learning_rate': (0.01, 0.1),\n",
    "                      'n_estimators': (20, 100),\n",
    "                      'boosting_type': Categorical(('gbdt',)),\n",
    "                      'xgboost_dart_mode': Categorical((True,)),\n",
    "                      'min_child_weight': (1, 5),\n",
    "                      'subsample_freq': (1, 5),\n",
    "                      'colsample_bytree': (0.9, 1),\n",
    "                      'reg_alpha': (1, 5),\n",
    "                      'reg_lambda': (0.1, 0.5),\n",
    "                      'drop_rate': (0.01, 0.5),\n",
    "                      'scale_pos_weight': (1, 5),\n",
    "                      'num_leaves' : (15, 31)}\n",
    "    return skopt_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def produceBestLightgbmModel(traindf, datamapper, target,\n",
    "                             configspace,\n",
    "                             fig_path=None, seed=27):\n",
    "\n",
    "    param_grid1, param_grid2, param_grid3, param_grid4 = configspace\n",
    "\n",
    "    # datamapper transform\n",
    "    train = datamapper.fit_transform(traindf[traindf.columns.difference([target])])\n",
    "    train = np.array(train)\n",
    "    labels_train = traindf[target]\n",
    "\n",
    "    # running grid search to get the best parameter set\n",
    "    (best_subsample, best_estimators, best_learning_rate, best_max_depth,\n",
    "     best_min_child_weight, best_colsample_bytree, best_reg_alpha, best_reg_lambda) =\\\n",
    "        lightGBMGridSearch(train, labels_train,\n",
    "                           param_grid1, param_grid2, param_grid3, param_grid4)\n",
    "\n",
    "    # train a gbm using the best parameter set\n",
    "    lgbm_best = LGBMClassifier(n_estimators=best_estimators, learning_rate=best_learning_rate,\n",
    "                               max_depth=best_max_depth, min_child_weight=best_min_child_weight,\n",
    "                               subsample=best_subsample, colsample_bytree=best_colsample_bytree,\n",
    "                               reg_alpha=best_reg_alpha, reg_lambda=best_reg_lambda,\n",
    "                               objective='binary', nthread=-1,\n",
    "                               scale_pos_weight=1, seed=seed)\n",
    "    return lgbm_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def max_depth_space(feature_size):\n",
    "    if feature_size > 1000:\n",
    "        max_depth = range(5, 14, 2)\n",
    "    else:\n",
    "        max_depth = range(3, 10, 2)\n",
    "    return list(max_depth)\n",
    "\n",
    "\n",
    "# <api>\n",
    "def min_child_weight_space(train_size):\n",
    "    if train_size > 10000:\n",
    "        min_child_weight = range(3, 8, 1)\n",
    "    else:\n",
    "        min_child_weight = range(1, 6, 1)\n",
    "    return list(min_child_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "# train lightgbm to get best n_estimators\n",
    "def lightGBMTrainBestn_estimators(alg, train, labels_train,\n",
    "                                  early_stopping_rounds=50):  \n",
    "    alg.fit(train, labels_train)\n",
    "    best_iteration = alg.best_iteration\n",
    "    return best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def lightGBMGridSearch(train, labels_train,\n",
    "                       param_grid1, param_grid2, param_grid3, param_grid4, seed=27):\n",
    "    \"\"\"\n",
    "    lightGBM grid search routine\n",
    "    \"\"\"\n",
    "    estimator = LGBMClassifier(max_depth=3, min_child_weight=1,\n",
    "                               subsample_freq=1, learning_rate=0.1,\n",
    "                               n_estimators=param_grid1['n_estimators'][0],\n",
    "                               colsample_bytree=0.8,\n",
    "                               objective='binary',\n",
    "                               nthread=-1, scale_pos_weight=1, seed=27)\n",
    "\n",
    "    best_estimators = lightGBMTrainBestn_estimators(estimator, train, labels_train)\n",
    "\n",
    "    estimator1 = LGBMClassifier(n_estimators=best_estimators,\n",
    "                                learning_rate=0.1,\n",
    "                                objective='binary:logistic',\n",
    "                                nthread=-1, scale_pos_weight=1,\n",
    "                                seed=seed)\n",
    "    gsearch1 = GridSearchCV(estimator=estimator1,\n",
    "                            param_grid=param_grid2, scoring='roc_auc',\n",
    "                            n_jobs=1, iid=False, cv=5)\n",
    "    gsearch1.fit(train, labels_train)\n",
    "\n",
    "    best_parameters = gsearch1.best_estimator_.get_params()\n",
    "    best_max_depth = best_parameters[\"max_depth\"]\n",
    "    best_min_child_weight = best_parameters['min_child_weight']\n",
    "    best_subsample = best_parameters['subsample_freq']\n",
    "    best_colsample_bytree = best_parameters['colsample_bytree']\n",
    "    estimator2 = LGBMClassifier(n_estimators=best_estimators,\n",
    "                                learning_rate=0.1,\n",
    "                                max_depth=best_max_depth,\n",
    "                                min_child_weight=best_min_child_weight,\n",
    "                                subsample_freq=best_subsample,\n",
    "                                colsample_bytree=best_colsample_bytree,\n",
    "                                objective='binary',\n",
    "                                nthread=-1, scale_pos_weight=1,\n",
    "                                seed=seed)\n",
    "    gsearch2 = GridSearchCV(estimator=estimator2,\n",
    "                            param_grid=param_grid3, scoring='roc_auc',\n",
    "                            n_jobs=1, iid=False, cv=5)\n",
    "    gsearch2.fit(train, labels_train)\n",
    "\n",
    "    best_parameters = gsearch2.best_estimator_.get_params()\n",
    "    best_reg_alpha = best_parameters[\"reg_alpha\"]\n",
    "    best_reg_lambda = best_parameters[\"reg_lambda\"]\n",
    "    \n",
    "    estimator3 = LGBMClassifier(n_estimators=best_estimators,\n",
    "                                max_depth=best_max_depth,\n",
    "                                min_child_weight=best_min_child_weight,\n",
    "                                subsample_freq=best_subsample,\n",
    "                                colsample_bytree=best_colsample_bytree,\n",
    "                                reg_alpha=best_reg_alpha,\n",
    "                                reg_lambda=best_reg_lambda,\n",
    "                                objective='binary',\n",
    "                                nthread=-1, scale_pos_weight=1,\n",
    "                                seed=seed),\n",
    "    gsearch3 = GridSearchCV(estimator=estimator3,\n",
    "                            param_grid=param_grid4, scoring='roc_auc',\n",
    "                            n_jobs=1, iid=False, cv=5)\n",
    "    gsearch3.fit(train, labels_train)\n",
    "\n",
    "    best_parameters = gsearch3.best_estimator_.get_params()\n",
    "    best_learning_rate = best_parameters[\"learning_rate\"]\n",
    "\n",
    "    estimator = LGBMClassifier(n_estimators=param_grid1['n_estimators'][0]*2,\n",
    "                               learning_rate=best_learning_rate,\n",
    "                               max_depth=best_max_depth,\n",
    "                               min_child_weight=best_min_child_weight,\n",
    "                               subsample_freq=best_subsample,\n",
    "                               colsample_bytree=best_colsample_bytree,\n",
    "                               reg_alpha=best_reg_alpha,\n",
    "                               reg_lambda=best_reg_lambda,\n",
    "                               objective='binary', nthread=-1,\n",
    "                               scale_pos_weight=1, seed=seed)\n",
    "    best_estimators = lightGBMTrainBestn_estimators(estimator, train, labels_train)\n",
    "\n",
    "    return (best_subsample, best_estimators, best_learning_rate, best_max_depth,\n",
    "            best_min_child_weight, best_colsample_bytree, best_reg_alpha, best_reg_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def produceBestModel(traindf, datamapper, target, configspace, fig_path=None, seed=27):\n",
    "    return produceBestLightgbmModel(traindf, datamapper, target, configspace, fig_path, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def optimizeBestModel(traindf, datamapper, target,\n",
    "                      configspace, search_alg,\n",
    "                      fig_path=None, n_calls=100,\n",
    "                      verbose=0, seed=27):\n",
    "    # datamapper transform\n",
    "    train = datamapper.fit_transform(traindf[traindf.columns.difference([target])])\n",
    "    train = np.array(train)\n",
    "    labels_train = traindf[target]\n",
    "    labels_train = np.array(labels_train)\n",
    "\n",
    "    # running grid search to get the best parameter set\n",
    "    best_params, trace = modelfit.searchBestParamsSkopt(train, labels_train,\n",
    "                                                        configspace, search_alg,\n",
    "                                                        LGBMClassifier, n_calls)\n",
    "    # train a gbm using the best parameter set\n",
    "    lgbm_best = LGBMClassifier(boosting_type='gbdt',\n",
    "                               n_estimators=best_params['n_estimators'],\n",
    "                               learning_rate=best_params['learning_rate'],\n",
    "                               max_depth=best_params['max_depth'],\n",
    "                               min_child_weight=best_params['min_child_weight'],\n",
    "                               subsample_freq=best_params['subsample_freq'],\n",
    "                               colsample_bytree=best_params['colsample_bytree'],\n",
    "                               reg_alpha=best_params['reg_alpha'],\n",
    "                               reg_lambda=best_params['reg_lambda'],\n",
    "                               scale_pos_weight=best_params['scale_pos_weight'],\n",
    "                               objective='binary',\n",
    "                               nthread=-1, seed=seed)\n",
    "\n",
    "    return lgbm_best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
