{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import work.marvin.binary_classifier_models.modelfit as modelfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def bestModelProducer(data, target, datamapper, fig_path=None):\n",
    "    \"\"\"\n",
    "    # auto GBDT model generation, 3 steps:\n",
    "    1. estimate optimal model parameters space for gridsearch,\n",
    "    depends on sample size and feature size\n",
    "    2. run gridsearch to find best parameter set\n",
    "    3. train the best GBDT model using the best parameter set\n",
    "    \"\"\"\n",
    "    traindf, testdf = modelfit.prepareDataforTraining(data)\n",
    "    train = datamapper.fit_transform(traindf[traindf.columns.difference([target])])\n",
    "    \n",
    "    # estimate optimal parameters grid space\n",
    "    configspace = parameterGridInitialization(datamapper.shape)\n",
    "    bestModel = produceBestGBMmodel(traindf, datamapper, target, configspace, figpath)\n",
    "    return bestModel, traindf, testdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def produceBestGBMmodel(traindf, datamapper, target, configspace, fig_path, seed, verbose):\n",
    "    \n",
    "    param_grid1, param_grid2 = configspace\n",
    "    \n",
    "    # datamapper transform\n",
    "    train = datamapper.fit_transform(traindf[traindf.columns.difference([target])])\n",
    "    labels_train = traindf[target]\n",
    "\n",
    "    # running grid search to get the best parameter set\n",
    "    (best_subsample, best_estimators, best_learning_rate, best_max_depth,\n",
    "     best_max_feature, best_min_samples_split) = gbmGridSearch(train,\n",
    "                                                               labels_train,\n",
    "                                                               param_grid1,\n",
    "                                                               param_grid2)\n",
    "\n",
    "    # train a gbm using the best parameter set\n",
    "    gbm_best = GradientBoostingClassifier(learning_rate=best_learning_rate,\n",
    "                                          n_estimators=best_estimators,\n",
    "                                          max_depth=best_max_depth,\n",
    "                                          min_samples_split=best_min_samples_split,\n",
    "                                          subsample=best_subsample,\n",
    "                                          max_features=best_max_feature,\n",
    "                                          random_state=seed)\n",
    "    return gbm_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def n_estimators_space(train_size):\n",
    "    if train_size > 10000:\n",
    "        n_estimators_spc = range(200, 1001, 200)\n",
    "    else:\n",
    "        n_estimators_spc = range(50, 201, 50)\n",
    "    return list(n_estimators_spc)\n",
    "\n",
    "# <api>\n",
    "def min_samples_split_space(train_size):\n",
    "    return list(range(min(train_size, 100), min(train_size, 601), 100))\n",
    "\n",
    "# <api>\n",
    "def max_feature_space(feature_size):\n",
    "    fs_sqrt = math.sqrt(feature_size)\n",
    "    if fs_sqrt > 10:\n",
    "        max_feature = range(int(fs_sqrt - 3), int(fs_sqrt * 1.50), 2)\n",
    "    else:\n",
    "        max_feature = range(int(fs_sqrt), int(fs_sqrt * 1.50), 2)\n",
    "    return list(max_feature)\n",
    "\n",
    "# <api>\n",
    "def max_depth_space(feature_size):\n",
    "    return [3, 5, 7, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def parameterGridInitialization(shape):\n",
    "    feature_size = shape[1] - 1\n",
    "    train_size = shape[0]\n",
    "\n",
    "    subsample_spc = [0.6, 0.7, 0.8, 0.9]\n",
    "    learning_rate_spc = [0.01, 0.05, 0.1]\n",
    "    n_estimators_spc = n_estimators_space(train_size)\n",
    "    min_samples_split_spc = min_samples_split_space(train_size)\n",
    "    max_feature_spc = max_feature_space(feature_size)\n",
    "    max_depth_spc = max_depth_space(feature_size)\n",
    "    min_samples_split_spc = min_samples_split_space(train_size)\n",
    "\n",
    "    # most important parameters\n",
    "    param_grid1 = {'subsample': subsample_spc, 'n_estimators': n_estimators_spc,\n",
    "                   'learning_rate': learning_rate_spc}\n",
    "    # tree specific parameters\n",
    "    param_grid2 = {'max_depth': max_depth_spc, 'max_features': max_feature_spc,\n",
    "                   'min_samples_split': min_samples_split_spc}\n",
    "\n",
    "    return param_grid1, param_grid2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def configSpaceInitialization(shape):\n",
    "    feature_size = shape[1] - 1\n",
    "    train_size = shape[0]\n",
    "\n",
    "    if train_size >= 1000:\n",
    "        skopt_grid = {'max_features': (2, feature_size),\n",
    "                      'max_depth': (2, 9),\n",
    "                      'learning_rate': (0.01, 0.2),\n",
    "                      'min_samples_split': (50, 500),\n",
    "                      'n_estimators': (50, 800),\n",
    "                      'subsample': (0.2, 0.9)}\n",
    "    else:\n",
    "        skopt_grid = {'max_features': (2, feature_size),\n",
    "                      'max_depth': (2, 9),\n",
    "                      'learning_rate': (0.01, 0.2),\n",
    "                      'min_samples_split': (20, train_size),\n",
    "                      'n_estimators': (50, 800),\n",
    "                      'subsample': (0.2, 0.9)}\n",
    "    return skopt_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def gbmGridSearch(train, labels_train, param_grid1, param_grid2, seed=27):\n",
    "    gsearch1 = GridSearchCV(estimator=GradientBoostingClassifier(min_samples_split=30,\n",
    "                                                                 max_features='sqrt',\n",
    "                                                                 max_depth=5,\n",
    "                                                                 random_state=10),\n",
    "                            param_grid=param_grid1, scoring='roc_auc',\n",
    "                            n_jobs=-1, pre_dispatch='2*n_jobs', iid=False, cv=5)\n",
    "    gsearch1.fit(train, labels_train)\n",
    "\n",
    "    best_parameters = gsearch1.best_estimator_.get_params()\n",
    "    best_subsample = best_parameters[\"subsample\"]\n",
    "    best_estimators = best_parameters['n_estimators']\n",
    "    best_learning_rate = best_parameters['learning_rate']\n",
    "\n",
    "    gsearch2 = GridSearchCV(estimator=GradientBoostingClassifier(subsample=best_subsample,\n",
    "                                                                 n_estimators=best_estimators,\n",
    "                                                                 learning_rate=best_learning_rate,\n",
    "                                                                 random_state=seed),\n",
    "                            param_grid=param_grid2, scoring='roc_auc',\n",
    "                            n_jobs=-1, pre_dispatch='2*n_jobs', iid=False, cv=5)\n",
    "    gsearch2.fit(train, labels_train)\n",
    "\n",
    "    best_parameters2 = gsearch2.best_estimator_.get_params()\n",
    "    best_max_depth = best_parameters2[\"max_depth\"]\n",
    "    best_max_feature = best_parameters2[\"max_features\"]\n",
    "    best_min_samples_split = best_parameters2[\"min_samples_split\"]\n",
    "\n",
    "    return (best_subsample, best_estimators, best_learning_rate,\n",
    "            best_max_depth, best_max_feature, best_min_samples_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def produceBestModel(traindf, datamapper, target, configspace, fig_path=None, seed=27, verbose=0):\n",
    "    return produceBestGBMmodel(traindf, datamapper, target, configspace, fig_path, seed, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def optimizeBestModel(traindf, datamapper, target,\n",
    "                      configspace, search_alg,\n",
    "                      fig_path=None, n_calls=100,\n",
    "                      verbose=0, seed=27):\n",
    "    # datamapper transform\n",
    "    train = datamapper.fit_transform(traindf[traindf.columns.difference([target])])\n",
    "    labels_train = np.array(traindf[target])\n",
    "\n",
    "    # running skopt.gbrt_search to get the best parameter set\n",
    "    best_params, trace = modelfit.searchBestParamsSkopt(train, labels_train,\n",
    "                                                        configspace, search_alg,\n",
    "                                                        GradientBoostingClassifier, n_calls)\n",
    "    \n",
    "    # search_alg: skopt_gbrt_search, skopt_gp_search, skopt_forest_search\n",
    "    gbdt_best = GradientBoostingClassifier(learning_rate=best_params['learning_rate'],\n",
    "                                           n_estimators=best_params['n_estimators'],\n",
    "                                           max_depth=best_params['max_depth'],\n",
    "                                           min_samples_split=best_params['min_samples_split'],\n",
    "                                           subsample=best_params['subsample'],\n",
    "                                           max_features=best_params['max_features'],\n",
    "                                           random_state=seed)\n",
    "    return gbdt_best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
