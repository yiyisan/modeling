{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn import metrics  # Additional scklearn functions\n",
    "from sklearn.ensemble import GradientBoostingClassifier  # GBM modelorithm\n",
    "from sklearn.grid_search import GridSearchCV  # Perforing grid search\n",
    "import work.marvin.binary_classifier_models.modelfit as modelfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def bestModelProducer(df, target, datamapper, fig_path):\n",
    "    \"\"\"\n",
    "    # auto GBDT model generation, 3 steps:\n",
    "    1. estimate optimal model parameters space for gridsearch,\n",
    "    depends on sample size and feature size\n",
    "    2. run gridsearch to find best parameter set\n",
    "    3. train the best GBDT model using the best parameter set\n",
    "    \"\"\"\n",
    "    traindf, testdf = modelfit.prepareDataforTraining(df, datamapper)\n",
    "    train_array = datamapper.transform(traindf)\n",
    "    train = train_array[:, :-1]\n",
    "    # estimate optimal parameters grid space\n",
    "    param_grid1, param_grid2 = parameterGridInitialization(train)\n",
    "    bestModel, accuracy, auc, cv_score = produceBestGBMmodel(traindf, testdf, datamapper,\n",
    "                                                             param_grid1, param_grid2, fig_path)\n",
    "    return bestModel, traindf, testdf, accuracy, auc, cv_score\n",
    "\n",
    "\n",
    "def initializationGridSearch(df, datamapper):\n",
    "    \"\"\"\n",
    "     根据特征数量、样本数量初始化GBM参数空间\n",
    "    \"\"\"\n",
    "    traindf, testdf = modelfit.prepareDataforTraining(df, datamapper)\n",
    "    train_array = datamapper.transform(traindf)\n",
    "    train = train_array[:, :-1]\n",
    "    # estimate optimal parameters grid space\n",
    "    param_grid1, param_grid2 = parameterGridInitialization(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def produceBestGBMmodel(traindf, testdf, datamapper,\n",
    "                        param_grid1, param_grid2,\n",
    "                        fig_path=None, seed=27):\n",
    "    # datamapper transform\n",
    "    train_array = datamapper.transform(traindf)\n",
    "    train = train_array[:, :-1]\n",
    "    labels_train = train_array[:, -1]\n",
    "\n",
    "    test_array = datamapper.transform(testdf)\n",
    "    test = test_array[:, :-1]\n",
    "    labels_test = test_array[:, -1]\n",
    "\n",
    "    # running grid search to get the best parameter set  \n",
    "    (best_subsample, best_estimators, best_learning_rate, best_max_depth,\n",
    "     best_max_feature, best_min_samples_split) = gbmGridSearch(train,\n",
    "                                                               labels_train,\n",
    "                                                               param_grid1,\n",
    "                                                               param_grid2)\n",
    "\n",
    "    # train a gbm using the best parameter set\n",
    "    gbm_best = GradientBoostingClassifier(learning_rate=best_learning_rate,\n",
    "                                          n_estimators=best_estimators,\n",
    "                                          max_depth=best_max_depth,\n",
    "                                          min_samples_split=best_min_samples_split,\n",
    "                                          subsample=best_subsample,\n",
    "                                          max_features=best_max_feature,\n",
    "                                          random_state=seed)\n",
    "\n",
    "    (alg, train_predictions,\n",
    "     train_predprob, cv_score) = modelfit.modelfit(gbm_best, datamapper,\n",
    "                                                  train, labels_train,\n",
    "                                                  test, labels_test,\n",
    "                                                  fig_path=fig_path)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(labels_train, train_predictions)\n",
    "    auc = metrics.roc_auc_score(labels_train, train_predprob)\n",
    "    cv_score = [np.mean(cv_score), np.std(cv_score), np.min(cv_score), np.max(cv_score)]\n",
    "\n",
    "    return alg, accuracy, auc, cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def n_estimators_space(train_size):\n",
    "    if train_size > 10000:\n",
    "        n_estimators_spc = range(200, 1001, 200)\n",
    "    else:\n",
    "        n_estimators_spc = range(50, 201, 50)\n",
    "    return list(n_estimators_spc)\n",
    "\n",
    "\n",
    "def min_samples_split_space(train_size):\n",
    "    return list(range(min(train_size, 100), min(train_size, 601), 100))\n",
    "\n",
    "\n",
    "def max_feature_space(feature_size):\n",
    "    fs_sqrt = math.sqrt(feature_size)\n",
    "    if fs_sqrt > 10:\n",
    "        max_feature = range(int(fs_sqrt - 3), int(fs_sqrt * 1.50), 2)\n",
    "    else:\n",
    "        max_feature = range(int(fs_sqrt), int(fs_sqrt * 1.50), 2)    \n",
    "    return list(max_feature)\n",
    "\n",
    "\n",
    "def max_depth_space(feature_size):\n",
    "    return [3, 5, 7, 9]\n",
    "\n",
    "\n",
    "def parameterGridInitialization(trainX):\n",
    "    feature_size = trainX.shape[1] - 1\n",
    "    train_size = trainX.shape[0]\n",
    "\n",
    "    subsample_spc = [0.6, 0.7, 0.8, 0.9]\n",
    "    learning_rate_spc = [0.01, 0.05, 0.1]\n",
    "    n_estimators_spc = n_estimators_space(train_size)\n",
    "    min_samples_split_spc = min_samples_split_space(train_size)\n",
    "    max_feature_spc = max_feature_space(feature_size)\n",
    "    max_depth_spc = max_depth_space(feature_size)\n",
    "    min_samples_split_spc = min_samples_split_space(train_size)\n",
    "\n",
    "    # most important parameters\n",
    "    param_grid1 = {'subsample': subsample_spc, 'n_estimators': n_estimators_spc,\n",
    "                   'learning_rate': learning_rate_spc}\n",
    "    # tree specific parameters\n",
    "    param_grid2 = {'max_depth': max_depth_spc, 'max_features': max_feature_spc,\n",
    "                   'min_samples_split': min_samples_split_spc}\n",
    "\n",
    "    return param_grid1, param_grid2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def configSpaceInitialization(trainX):\n",
    "    feature_size = trainX.shape[1] - 1\n",
    "    train_size = trainX.shape[0]\n",
    "\n",
    "    if train_size >= 1000:\n",
    "        skopt_grid = {'max_features': (2, feature_size),\n",
    "                      'max_depth': (2, 9),\n",
    "                      'learning_rate': (0.01, 0.2),\n",
    "                      'min_samples_split': (50, 500),\n",
    "                      'n_estimators': (50, 800),\n",
    "                      'subsample': (0.2, 0.9)}\n",
    "    else:\n",
    "        skopt_grid = {'max_features': (2, feature_size),\n",
    "                      'max_depth': (2, 9),       \n",
    "                      'learning_rate': (0.01, 0.2),\n",
    "                      'min_samples_split': (20, train_size),\n",
    "                      'n_estimators': (50, 800),\n",
    "                      'subsample': (0.2, 0.9)}\n",
    "    return skopt_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def searchBestParamsSkopt(train, labels_train, skopt_grid, search_alg, n_calls=100):\n",
    "    experiment_setting = [(search_alg, skopt_grid, {'n_calls': n_calls})]\n",
    "    experiment_result = modelfit.run_experiments(experiment_setting,\n",
    "                                                 train, labels_train,\n",
    "                                                 GradientBoostingClassifier)\n",
    "    test_accuracy = experiment_result[0]['Test accuracy']\n",
    "    max_index = test_accuracy.index(max(test_accuracy))\n",
    "    best_params = experiment_result[0]['Best parameters'][max_index]\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def produceBestModel(traindf, testdf, datamapper, param_grid, fig_path=None, seed=27):\n",
    "    param_grid1, param_grid2 = param_grid\n",
    "    return produceBestGBMmodel(traindf, testdf, datamapper,\n",
    "                               param_grid1, param_grid2,\n",
    "                               fig_path, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def gbmGridSearch(train, labels_train, param_grid1, param_grid2, seed=27):\n",
    "    gsearch1 = GridSearchCV(estimator=GradientBoostingClassifier(min_samples_split=30,\n",
    "                                                                 max_features='sqrt',\n",
    "                                                                 max_depth=5,\n",
    "                                                                 random_state=10),\n",
    "                            param_grid=param_grid1, scoring='roc_auc',\n",
    "                            n_jobs=-1, pre_dispatch='2*n_jobs', iid=False, cv=5)\n",
    "    gsearch1.fit(train, labels_train)\n",
    "\n",
    "    best_parameters = gsearch1.best_estimator_.get_params()\n",
    "    best_subsample = best_parameters[\"subsample\"]\n",
    "    best_estimators = best_parameters['n_estimators']\n",
    "    best_learning_rate = best_parameters['learning_rate']\n",
    "\n",
    "    gsearch2 = GridSearchCV(estimator=GradientBoostingClassifier(subsample=best_subsample,\n",
    "                                                                 n_estimators=best_estimators,\n",
    "                                                                 learning_rate=best_learning_rate,\n",
    "                                                                 random_state=seed),\n",
    "                            param_grid=param_grid2, scoring='roc_auc',\n",
    "                            n_jobs=-1, pre_dispatch='2*n_jobs', iid=False, cv=5)\n",
    "    gsearch2.fit(train, labels_train)\n",
    "\n",
    "    best_parameters2 = gsearch2.best_estimator_.get_params()\n",
    "    best_max_depth = best_parameters2[\"max_depth\"]\n",
    "    best_max_feature = best_parameters2[\"max_features\"]\n",
    "    best_min_samples_split = best_parameters2[\"min_samples_split\"]\n",
    "\n",
    "    return (best_subsample, best_estimators, best_learning_rate,\n",
    "            best_max_depth, best_max_feature, best_min_samples_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def optimizeBestModel(traindf, testdf, datamapper,\n",
    "                      configspace, search_alg,\n",
    "                      fig_path=None, n_calls=100, seed=27):\n",
    "    # datamapper transform\n",
    "    train_array = datamapper.transform(traindf)\n",
    "    train = train_array[:, :-1]            # 默认label为最后一列\n",
    "    labels_train = train_array[:, -1]      # 默认label为最后一列\n",
    "    test_array = datamapper.transform(testdf)\n",
    "    test = test_array[:, :-1]\n",
    "    labels_test = test_array[:, -1]\n",
    "\n",
    "    # running skopt.gbrt_search to get the best parameter set\n",
    "    # search_alg: skopt_gbrt_search, skopt_gp_search, skopt_forest_search\n",
    "    best_params = searchBestParamsSkopt(train, labels_train, configspace, search_alg, n_calls)\n",
    "\n",
    "    gbdt_best = GradientBoostingClassifier(learning_rate=best_params['learning_rate'],\n",
    "                                           n_estimators=best_params['n_estimators'],\n",
    "                                           max_depth=best_params['max_depth'],\n",
    "                                           min_samples_split=best_params['min_samples_split'],\n",
    "                                           subsample=best_params['subsample'],\n",
    "                                           max_features=best_params['max_features'],\n",
    "                                           random_state=seed)\n",
    "\n",
    "    alg, train_predictions, train_predprob, cv_score = modelfit.modelfit(gbdt_best, datamapper,\n",
    "                                                                         train, labels_train,\n",
    "                                                                         test, labels_test,\n",
    "                                                                         fig_path)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(labels_train, train_predictions)\n",
    "    auc = metrics.roc_auc_score(labels_train, train_predprob)\n",
    "    cv_score = [np.mean(cv_score), np.std(cv_score), np.min(cv_score), np.max(cv_score)]\n",
    "\n",
    "    return alg, accuracy, auc, cv_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
