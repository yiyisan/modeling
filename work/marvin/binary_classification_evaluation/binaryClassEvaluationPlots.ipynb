{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def algEvaluateOnTestSet(alg, testdf, datamapper, ks_fig_path, sub_fig_path):\n",
    "\n",
    "    test_array = datamapper.transform(testdf)\n",
    "    test = test_array[:, :-1]\n",
    "    labels_test = test_array[:, -1]\n",
    "\n",
    "    test_predprob = applyAlgOnTestSet(alg, test)\n",
    "    try:\n",
    "        # plots: KS, ROC, precision_recall, precision_cutoff, recall_cutoff\n",
    "        ks_val, ks_x, p, r = ks_curve(labels_test, test_predprob, ks_fig_path)\n",
    "    except IndexError:\n",
    "        logger.error(\"this data can't fit a KS-Curve\")\n",
    "        ks_val, ks_x, p, r = 0., 0., 0., 0.\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    roc_curve_plot(labels_test, test_predprob)\n",
    "    plt.subplot(2, 2, 2)\n",
    "    precision_recall_curve_plot(labels_test, test_predprob)\n",
    "    plt.subplot(2, 2, 3)\n",
    "    precision_cutoff_curve(labels_test, test_predprob)\n",
    "    plt.subplot(2, 2, 4)\n",
    "    recall_cutoff_curve(labels_test, test_predprob)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(sub_fig_path)\n",
    "\n",
    "    return test_predprob, ks_val, ks_x, p, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def applyAlgOnTestSet(alg, test):\n",
    "    test_predprob = alg.predict_proba(test)[:, 1]\n",
    "    return test_predprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def greaterThan(a, b):\n",
    "    return 1 if a > b else 0\n",
    "\n",
    "\n",
    "def dividZeroProcess(a, b):\n",
    "    if b == 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return a * 1. / b\n",
    "\n",
    "\n",
    "def gen_pro_quantile(grp_cnt):\n",
    "    piece = int(100 / grp_cnt)\n",
    "\n",
    "    i = piece\n",
    "    quantile = []\n",
    "\n",
    "    while i <= 100:\n",
    "        quantile.append(str(i) + '%')\n",
    "        i += piece\n",
    "    return quantile\n",
    "\n",
    "\n",
    "def metricsPlot(x, y, xlab, ylab, title):\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(xlab)\n",
    "    plt.ylabel(ylab)\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "\n",
    "def prec(Y_true, Y_predprob, t):\n",
    "    vfunc = np.vectorize(greaterThan)\n",
    "    return metrics.precision_score(Y_true, vfunc(Y_predprob, t))\n",
    "\n",
    "\n",
    "def rec(Y_true, Y_predprob, t):\n",
    "    vfunc = np.vectorize(greaterThan)\n",
    "    return metrics.recall_score(Y_true, vfunc(Y_predprob, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def roc_curve_plot(Y_true, Y_predprob):\n",
    "    from sklearn.metrics import roc_curve\n",
    "    fpr, tpr, thresh = roc_curve(Y_true, Y_predprob)\n",
    "    auc = round(auc_calculate(Y_true, Y_predprob), 4)\n",
    "    plt.plot(fpr, tpr, label=\"ROC-AUC overall, \\n AUC Score={}\".format(auc))\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.plot([0, 1], [0, 1])\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "\n",
    "def precision_recall_curve_plot(Y_true, Y_predprob):\n",
    "    precision, recall, threshold = precision_recall_curve(Y_true, Y_predprob)\n",
    "    metricsPlot(precision, recall,\n",
    "                \"Precision\", \"Recall\", \"Precision Vs Recall Curve\")\n",
    "\n",
    "\n",
    "def precision_cutoff_curve(Y_true, Y_predprob):\n",
    "    vfunc = np.vectorize(greaterThan)\n",
    "    max_t = max(Y_predprob)\n",
    "    t = []\n",
    "    prec = []\n",
    "    for i in range(0, 101, 1):\n",
    "        if i / 100. <= max_t:\n",
    "            t.append(i / 100.)\n",
    "            prec.append(metrics.precision_score(Y_true, vfunc(Y_predprob, t[i])))\n",
    "    metricsPlot(t, prec, \"cut-off\", \"Precision\", \"Precision Vs Cut-off Curve\")\n",
    "\n",
    "\n",
    "def recall_cutoff_curve(Y_true, Y_predprob):\n",
    "    vfunc = np.vectorize(greaterThan)\n",
    "    max_t = max(Y_predprob)\n",
    "    t = []\n",
    "    rec = []\n",
    "    for i in range(0, 101, 1):\n",
    "        if i / 100. <= max_t:\n",
    "            t.append(i / 100.)\n",
    "            rec.append(metrics.recall_score(Y_true, vfunc(Y_predprob, t[i])))\n",
    "    metricsPlot(t, rec, \"cut-off\", \"Recall\", \"Recall Vs Cut-off Curve\")\n",
    "\n",
    "\n",
    "def auc_calculate(Y_true, Y_predprob):\n",
    "    fpr, tpr, thresh = roc_curve(Y_true, Y_predprob)\n",
    "    return round(metrics.auc(fpr, tpr), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def ks_curve(Y_true, Y_predprob, fig_path):\n",
    "    # Kolmogorov-Smirnov Test\n",
    "    df = pd.DataFrame({'Y_truth': Y_true, 'Y_predprob_1': Y_predprob})\n",
    "    a = df[df.Y_truth == 0]['Y_predprob_1']\n",
    "    b = df[df.Y_truth == 1]['Y_predprob_1']\n",
    "\n",
    "    a1 = a.reset_index(drop=True)\n",
    "    b1 = b.reset_index(drop=True)\n",
    "\n",
    "    data1, data2 = map(np.asarray, (a1, b1))\n",
    "    n1 = data1.shape[0]\n",
    "    n2 = data2.shape[0]\n",
    "    n1 = len(data1)\n",
    "    n2 = len(data2)\n",
    "    data1 = np.sort(data1)\n",
    "    data2 = np.sort(data2)\n",
    "\n",
    "    data_all = np.concatenate([data1, data2])\n",
    "    cdf1 = np.searchsorted(data1, data_all, side='right') / (1. * n1)\n",
    "    cdf2 = np.searchsorted(data2, data_all, side='right') / (1. * n2)\n",
    "\n",
    "    cdf_abs_dif = np.absolute(cdf1 - cdf2)\n",
    "    d = np.max(cdf_abs_dif)\n",
    "\n",
    "    pos1 = -1\n",
    "    pos2 = -1\n",
    "    for i in range(len(cdf_abs_dif)):\n",
    "        if np.isclose(d, cdf_abs_dif[i]):\n",
    "            pos1, pos2 = cdf1[i], cdf2[i]\n",
    "            break\n",
    "\n",
    "    y_1 = np.arange(n1) / float(n1)\n",
    "    y_2 = np.arange(n2) / float(n2)\n",
    "\n",
    "    x_1_idx = []\n",
    "    for i in range(len(y_1)):\n",
    "        if np.isclose(pos1, y_1[i]):\n",
    "            x_1_idx.append(i)\n",
    "            break\n",
    "    x_2_idx = []\n",
    "    for i in range(len(y_2)):\n",
    "        if np.isclose(pos2, y_2[i]):\n",
    "            x_2_idx.append(i)\n",
    "            break\n",
    "\n",
    "    x_0 = (data1[x_1_idx[0]] + data2[x_2_idx[0]]) / 2.\n",
    "\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(data1, y_1, label=\"good sample\")\n",
    "    plt.plot(data2, y_2, label=\"bad sample\")\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([x_0, x_0], [y_1[x_1_idx], y_2[x_2_idx]], linestyle=\"--\")\n",
    "    plt.scatter([x_0, x_0], [y_1[x_1_idx], y_2[x_2_idx]], 50, color='orange')\n",
    "    plt.xlim([-0.01, 1.01])\n",
    "    plt.ylim([-0.01, 1.01])\n",
    "    plt.xlabel('Probability')\n",
    "    plt.ylabel('F_n(Probability)')\n",
    "    plt.title('Kolmogorov - Smirnov Chart')\n",
    "    plt.savefig(fig_path)\n",
    "\n",
    "    return (d, round(x_0, 2),\n",
    "            round(prec(Y_true, Y_predprob, round(x_0, 2)), 2),\n",
    "            round(rec(Y_true, Y_predprob, round(x_0, 2)), 2))\n",
    "\n",
    "\n",
    "def confusionMatrixs(Y_true, Y_predprob, cut_off):\n",
    "    vfunc = np.vectorize(greaterThan)\n",
    "    y_pred = vfunc(Y_predprob, cut_off)\n",
    "    return confusion_matrix(Y_true, y_pred)\n",
    "\n",
    "\n",
    "def keyClassificationMetrics(Y_true, Y_predprob, cut_off):\n",
    "    vfunc = np.vectorize(greaterThan)\n",
    "    y_pred = vfunc(Y_predprob, cut_off)\n",
    "    target_names = ['class 0', 'class 1']\n",
    "    return classification_report(Y_true, y_pred, target_names=target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def GBoddsWRTpredprob(Y_true, Y_predprob, groupCount=10):\n",
    "    \"\"\"\n",
    "    groupCount: 10,20\n",
    "    should return a table with GBodds per predprob group\n",
    "    (即把predprob按照quantile分为10或者20组，每组的good/bad ratio)\n",
    "    \"\"\"\n",
    "    test = pd.DataFrame({'label': Y_true, 'predprob': Y_predprob})\n",
    "    good = []\n",
    "    bad = []\n",
    "    oddsRatio = []\n",
    "    if groupCount != 10 and groupCount != 20:\n",
    "        groupCount = 10\n",
    "    quant = []\n",
    "    for i in range(1, groupCount + 1):\n",
    "        quant.append(test['predprob'].quantile(1. / groupCount * i))\n",
    "    for i in range(1, groupCount + 1):\n",
    "        if i == 1:\n",
    "            pro_1 = quant[i - 1]\n",
    "            df = test[test.predprob <= pro_1]\n",
    "        else:\n",
    "            pro_0 = quant[i - 2]\n",
    "            pro_1 = quant[i - 1]\n",
    "            df1 = test[test.predprob > pro_0]\n",
    "            df = df1[df1.predprob <= pro_1]\n",
    "        gcount = df[df.label == 0].shape[0]\n",
    "        bcount = df[df.label == 1].shape[0]     \n",
    "        good.append(gcount)\n",
    "        bad.append(bcount)\n",
    "        oddsRatio.append(dividZeroProcess(gcount, bcount))\n",
    "    oddsDF = pd.DataFrame({'proQuantile': gen_pro_quantile(groupCount),\n",
    "                           'Prob_1': quant,\n",
    "                           'good_count': good, 'bad_count': bad, 'odds': oddsRatio})\n",
    "    return oddsDF[['proQuantile', 'Prob_1', 'good_count', 'bad_count', 'odds']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
