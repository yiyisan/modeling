{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "import scipy\n",
    "import sklearn.metrics as metrics\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy import interpolate \n",
    "from matplotlib.pylab import rcParams\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "# alg: current used model (parent_model)\n",
    "# alg2: new model iterated on incremental data (child_model)\n",
    "# testfresh: new model 训练过程中，划分train/test，testfresh: test里面并且是新增的数据 \n",
    "\n",
    "def applyTwoModelsOnDataset(alg_child, alg_parent, testdf, trn_d, parent_end_index, datamapper, cmpsub_fig_path):\n",
    "    \n",
    "    testdf_index = testdf.index.tolist()\n",
    "    fresh_testdf_index = [index for index in testdf_index if index >= parent_end_index]\n",
    "    fresh_testdf = trn_d.loc[fresh_testdf_index]\n",
    "    \n",
    "    test_array = datamapper.transform(fresh_testdf)\n",
    "    test = test_array[:, :-1]\n",
    "    labels_test = test_array[:, -1]\n",
    "  \n",
    "    prob_child = alg_child.predict_proba(test)[:,1]\n",
    "    prob_parent = alg_parent.predict_proba(test)[:,1]\n",
    "    \n",
    "    mainMetricsComparison((labels_test, prob_child), (labels_test, prob_parent), cmpsub_fig_path)\n",
    "    return prob_parent, prob_child\n",
    "    \n",
    "# <api>\n",
    "def mainMetricsComparison((labels_test,test_predprob),(labels_test1,test_predprob1), cmpsub_fig_path):\n",
    "    # plots: KS, ROC, precision_recall, precision_cutoff, recall_cutoff\n",
    "    rcParams['figure.figsize'] = 10, 10\n",
    "    plt.subplot(2,2,1)\n",
    "    roc_curve_plotXY((labels_test,test_predprob),(labels_test1,test_predprob1))\n",
    "    plt.subplot(2,2,2)\n",
    "    precision_recall_curve_plotXY((labels_test,test_predprob),(labels_test1,test_predprob1))\n",
    "    plt.subplot(2,2,3)\n",
    "    precision_cutoff_curve((labels_test,test_predprob),(labels_test1,test_predprob1))\n",
    "    plt.subplot(2,2,4)\n",
    "    recall_cutoff_curve((labels_test,test_predprob),(labels_test1,test_predprob1))\n",
    "    plt.savefig(cmpsub_fig_path)\n",
    "\n",
    "# <api>\n",
    "def greaterThan(a, b):\n",
    "    if a > b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0   \n",
    "    \n",
    "# <api>\n",
    "def metricsPlotXY(metricX, metricyY, xlab, ylab, title, loc):\n",
    "    (x,y) = metricX\n",
    "    (x1,y1) = metricyY\n",
    "    plt.plot(x, y, label = '1st classifier')\n",
    "    plt.plot(x1, y1, label = '2nd classifier')\n",
    "    plt.xlabel(xlab)\n",
    "    plt.ylabel(ylab)\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=loc)\n",
    "    #plt.show()\n",
    "    \n",
    "# <api>\n",
    "def prec(Y_true, Y_predprob, t): \n",
    "    vfunc = np.vectorize(greaterThan) \n",
    "    return metrics.precision_score(Y_true, vfunc(Y_predprob, t))\n",
    "\n",
    "# <api>\n",
    "def rec(Y_true, Y_predprob, t): \n",
    "    vfunc = np.vectorize(greaterThan) \n",
    "    return metrics.recall_score(Y_true, vfunc(Y_predprob, t))\n",
    "\n",
    "# <api>\n",
    "def auc_calculate(Y_true,Y_predprob):\n",
    "    fpr,tpr,thresh = roc_curve(Y_true,Y_predprob)\n",
    "    return round(metrics.auc(fpr, tpr), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def roc_curve_plotXY(testX, testY):\n",
    "    Y_trueX, Y_predprobX = testX\n",
    "    Y_trueY, Y_predprobY = testY\n",
    "    \n",
    "    from sklearn.metrics import roc_curve\n",
    "    fprX, tprX, threshX = roc_curve(Y_trueX,Y_predprobX)\n",
    "    fprY, tprY, threshY = roc_curve(Y_trueY,Y_predprobY)\n",
    "    \n",
    "    aucX = round(auc_calculate(Y_trueX,Y_predprobX),4)    \n",
    "    aucY = round(auc_calculate(Y_trueY,Y_predprobY),4)   \n",
    "    \n",
    "    plt.plot(fprX, tprX, label='ROC-AUC overall (1st),\\n AUC Score=' + str(aucX))\n",
    "    plt.plot(fprY, tprY, label='ROC-AUC overall (2nd),\\n AUC Score=' + str(aucY))\n",
    "    \n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.plot([0, 1], [0, 1])    \n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend(loc='lower right')\n",
    "    #plt.show()\n",
    "    \n",
    "# <api>\n",
    "def precision_recall_curve_plotXY(testX, testY):\n",
    "    Y_trueX, Y_predprobX = testX\n",
    "    Y_trueY, Y_predprobY = testY\n",
    "    \n",
    "    precisionX, recallX, thresholdX = precision_recall_curve(Y_trueX, Y_predprobX)\n",
    "    precisionY, recallY, thresholdY = precision_recall_curve(Y_trueY, Y_predprobY)\n",
    "    \n",
    "    metricsPlotXY((precisionX,recallX), (precisionY,recallY), \"Precision\", \"Recall\", \"Precision Vs Recall Curve\", 'upper right')\n",
    "    \n",
    "# <api>\n",
    "def precision_cutoff_curve(testX, testY):\n",
    "    Y_trueX, Y_predprobX = testX\n",
    "    Y_trueY, Y_predprobY = testY\n",
    "    \n",
    "    vfunc = np.vectorize(greaterThan) \n",
    "    max_tX = max(Y_predprobX)\n",
    "    tX = []\n",
    "    precX = []\n",
    "    for i in range(0, 101, 1):\n",
    "        if i / 100.0000 <= max_tX:\n",
    "            tX.append(i / 100.0000)\n",
    "            precX.append(metrics.precision_score(Y_trueX, vfunc(Y_predprobX, tX[i])))\n",
    "            \n",
    "    max_tY = max(Y_predprobY)\n",
    "    tY = []\n",
    "    precY = []\n",
    "    for i in range(0, 101, 1):\n",
    "        if i / 100.0000 <= max_tY:\n",
    "            tY.append(i / 100.0000)\n",
    "            precY.append(metrics.precision_score(Y_trueY, vfunc(Y_predprobY, tY[i])))\n",
    "            \n",
    "    metricsPlotXY((tX, precX), (tY, precY), \"cut-off\", \"Precision\", \"Precision Vs Cut-off Curve\", 'lower right')\n",
    "\n",
    "# <api>        \n",
    "def recall_cutoff_curve(testX, testY):\n",
    "    (Y_trueX, Y_predprobX) = testX\n",
    "    (Y_trueY, Y_predprobY) = testY\n",
    "    \n",
    "    vfunc = np.vectorize(greaterThan) \n",
    "    vfunc = np.vectorize(greaterThan) \n",
    "    max_tX = max(Y_predprobX)\n",
    "    tX = []\n",
    "    recallX = []\n",
    "    for i in range(0, 101, 1):\n",
    "        if i / 100.0000 <= max_tX:\n",
    "            tX.append(i / 100.0000)\n",
    "            recallX.append(metrics.recall_score(Y_trueX, vfunc(Y_predprobX, tX[i])))\n",
    "            \n",
    "    max_tY = max(Y_predprobY)\n",
    "    tY = []\n",
    "    recallY = []\n",
    "    for i in range(0, 101, 1):\n",
    "        if i / 100.0000 <= max_tY:\n",
    "            tY.append(i / 100.0000)\n",
    "            recallY.append(metrics.recall_score(Y_trueY, vfunc(Y_predprobY, tY[i])))\n",
    "    \n",
    "    metricsPlotXY((tX, recallX), (tY, recallY), \"cut-off\", \"Recall\", \"Recall Vs Cut-off Curve\", 'upper right')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
