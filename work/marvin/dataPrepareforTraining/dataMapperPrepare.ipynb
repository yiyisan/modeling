{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# <api>\n",
    "from sklearn2pmml.decoration import ContinuousDomain, CategoricalDomain, OrdinalDomain\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import Imputer, Normalizer\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer, OneHotEncoder\n",
    "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "import base64\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "try:\n",
    "    from exceptions import Exception\n",
    "except:\n",
    "    pass\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "class DataMapperError(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MisValFunc(Enum):\n",
    "    AS_NEW_CLASS = 'as new class'\n",
    "    DROP_ROW = 'drop row'\n",
    "    MEAN = 'mean'\n",
    "    DEFAULT = 'default'\n",
    "\n",
    "    def apply(self, ftr, data, val=None):\n",
    "        if self is MisValFunc.DEFAULT:\n",
    "            set_default_value(data, ftr, val)\n",
    "            return data\n",
    "        elif self is MisValFunc.MEAN:\n",
    "            set_mean(data, ftr)\n",
    "            return data\n",
    "        elif self is MisValFunc.DROP_ROW:\n",
    "            drop_row(data, ftr)\n",
    "            return data\n",
    "        elif self is MisValFunc.AS_NEW_CLASS:\n",
    "            newVal = set_as_new_class(data, ftr)\n",
    "            return (self.value, newVal)\n",
    "        else:\n",
    "            raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FtrTransFunc(Enum):\n",
    "    MIN_MAX_SCALER = 'MinMaxScaler'\n",
    "    STANDARD_SCALER = 'StandardScaler'\n",
    "    MAX_ABS_SCALER = 'MaxAbsScaler'\n",
    "    NORMALIZER = 'Normalizer'\n",
    "    BINARIZER = 'Binarizer'\n",
    "    ONE_HOT_ENCODER = 'OneHotEncoder'\n",
    "    NUMPY_LOG1P = 'NumPy.log1p'\n",
    "    NUMPY_LOG = 'NumPy.log'\n",
    "\n",
    "    @property\n",
    "    def method(self):\n",
    "        if self is FtrTransFunc.MIN_MAX_SCALER:\n",
    "            return MinMaxScaler(copy=False)\n",
    "        elif self is FtrTransFunc.STANDARD_SCALER:\n",
    "            return StandardScaler(copy=False)\n",
    "        elif self is FtrTransFunc.MAX_ABS_SCALER:\n",
    "            return MaxAbsScaler(copy=False)\n",
    "        elif self is FtrTransFunc.NORMALIZER:\n",
    "            return FunctionTransformer(Normalizer(axis=0), False)\n",
    "        elif self is FtrTransFunc.BINARIZER:\n",
    "            return LabelBinarizer(copy=False)\n",
    "        elif self is FtrTransFunc.ONE_HOT_ENCODER:\n",
    "            return OneHotEncoder()\n",
    "        elif self is FtrTransFunc.NUMPY_LOG1P:\n",
    "            return FunctionTransformer(np.log1p, False)\n",
    "        elif self is FtrTransFunc.NUMPY_LOG:\n",
    "            return FunctionTransformer(np.log, False)\n",
    "        else:\n",
    "            raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def b64_file_data(fig_path):\n",
    "    fig_data = None\n",
    "    with open(fig_path, 'r') as infile:\n",
    "        fig_data = infile.read()\n",
    "    return base64.b64encode(fig_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def drop_row(data, ftr):\n",
    "    data.dropna(how='any', subset=[ftr], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def set_as_new_class(data, ftr):\n",
    "    uniq_v = data[ftr].unique()\n",
    "    uniq_v = uniq_v[~pd.isnull(uniq_v)]\n",
    "    if 0 == len(uniq_v):\n",
    "        raise Exception('all values of ' + ftr + ' are nan')\n",
    "\n",
    "    # maybe need more check for the data type\n",
    "    v = uniq_v[0]\n",
    "    if isinstance(v, str):\n",
    "        new_v = ftr + '_newclass'\n",
    "    elif isinstance(v, (float, int)):\n",
    "        new_v = uniq_v.astype('float32').max() + 1\n",
    "        data[ftr] = data[ftr].astype('float32')\n",
    "    else:\n",
    "        raise Exception('categorical value is string or numerical?')\n",
    "    set_default_value(data, ftr, new_v)\n",
    "    data[ftr] = data[ftr].astype('category')\n",
    "    return new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def set_default_value(data, ftr, v):\n",
    "    if v is None:\n",
    "        raise Exception('value is None')\n",
    "\n",
    "    old_type = 'category'\n",
    "    if 'category' == data[ftr].dtype.name:\n",
    "        data[ftr] = data[ftr].astype('object', copy=True)\n",
    "        v = str(v)\n",
    "    else:\n",
    "        data[ftr] = data[ftr].astype('float32', copy=True)\n",
    "        v = float(v)\n",
    "        old_type = 'float32'\n",
    "\n",
    "    data[ftr].fillna(v, inplace=True)\n",
    "    data[ftr] = data[ftr].astype(old_type, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def set_mean(data, ftr):\n",
    "    series = data[ftr]\n",
    "    tmp = Imputer(axis=1).fit_transform(series)\n",
    "    data[ftr].update(pd.Series(tmp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def deal_missing_value(data, mis_val):\n",
    "    for f in mis_val.keys():\n",
    "        m = mis_val[f]\n",
    "\n",
    "        if MisValFunc.DROP_ROW == m:\n",
    "            drop_row(data, f)\n",
    "        elif MisValFunc.AS_NEW_CLASS == m:\n",
    "            nc = set_as_new_class(data, f)\n",
    "            mis_val[f] = (m, nc)\n",
    "        elif MisValFunc.MEAN == m:\n",
    "            set_mean(data, f)\n",
    "        elif isinstance(m, tuple) and 'default' == m[0]:\n",
    "            set_default_value(data, f, m[1])\n",
    "\n",
    "    return mis_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def move_target_last(data, target_col):\n",
    "    reindex_col = [c for c in data.columns]\n",
    "    if target_col not in reindex_col:\n",
    "        return data\n",
    "    reindex_col.remove(target_col)\n",
    "    reindex_col.append(target_col)\n",
    "    return data.reindex_axis(reindex_col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def is_binary(ftr_vlst):\n",
    "    uniq_sp = ftr_vlst.unique()\n",
    "    uniq_sp = uniq_sp[~pd.isnull(uniq_sp)]\n",
    "    return 2 == len(uniq_sp)\n",
    "\n",
    "\n",
    "def missing_ivt(mis_val, col):\n",
    "    if col not in mis_val:\n",
    "        return 'as_is'\n",
    "    if isinstance(mis_val[col], tuple) and 'mean' != mis_val[col][0]:\n",
    "        return 'as_missing'\n",
    "    elif 'drop row'.lower() == mis_val[col]:\n",
    "        return 'return_invalid'\n",
    "    else:\n",
    "        raise Exception(\"\"\"Invalid missing treatment\n",
    "        of feature: {}\"\"\".format(col))\n",
    "\n",
    "\n",
    "def onehot_encoder_with_missing(trn_series):\n",
    "    unary = (trn_series.unique()[0] == 1)\n",
    "    binary_with_na = is_binary(trn_series) and 'CreditX-NA' in set(trn_series)\n",
    "    if unary or binary_with_na:\n",
    "        return LabelEncoder()\n",
    "    else:\n",
    "        return LabelBinarizer()\n",
    "\n",
    "\n",
    "def continuous_feature_transform(ftr_trf, col):\n",
    "    prep = None\n",
    "    if col in ftr_trf:\n",
    "        if 'norm' == ftr_trf[col]:\n",
    "            prep = MinMaxScaler(copy=False)\n",
    "        elif 'log' == ftr_trf[col]:\n",
    "            prep = FunctionTransformer(np.log)\n",
    "        elif 'log1p' == ftr_trf[col]:\n",
    "            prep = FunctionTransformer(np.log1p, kw_args=None)\n",
    "        prep.name = ftr_trf[col]\n",
    "    return prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def dataMapperBuilder(trn_d, categ_ftr, conti_ftr, invalid_ftr=None, mis_val=None, ftr_trf=None):\n",
    "    \"\"\"\n",
    "    build dataFrameMapper according to colume type\n",
    "    trn_d: traning data in DataFrame format\n",
    "    categ_ftr: categorical feature(to_dummies)\n",
    "    conti_ftr: continuous feature(feature transformer)\n",
    "    mis_val: missing value treatment\n",
    "    ftr_trf: feature transformer\n",
    "    \"\"\"\n",
    "    invalid_ftr = invalid_ftr if invalid_ftr else []\n",
    "    mis_val = mis_val if mis_val else {}\n",
    "    ftr_trf = ftr_trf if ftr_trf else {}\n",
    "    c_map = []\n",
    "    for col in trn_d.columns:\n",
    "        prep = []\n",
    "        op_lst = []\n",
    "        if col in categ_ftr:\n",
    "            ivt = missing_ivt(mis_val, col)\n",
    "            if 'as_missing' == ivt:\n",
    "                encoder = onehot_encoder_with_missing(trn_d[col])\n",
    "                prep.append(encoder.fit(trn_d[col]))\n",
    "                missing_value_treatment = mis_val.get(col, ('asMode', None))[0]\n",
    "                missing_value_replacement = mis_val.get(col, (None, None))[1]\n",
    "                dom = CategoricalDomain(invalid_value_treatment=ivt,\n",
    "                                        invalid_default='CreditX-NA',\n",
    "                                        missing_value_treatment=missing_value_treatment,\n",
    "                                        missing_value_replacement=missing_value_replacement)\n",
    "            else:\n",
    "                encoder = LabelEncoder() if is_binary(trn_d[col]) else LabelBinarizer()\n",
    "                prep.append(encoder.fit(trn_d[col]))\n",
    "                dom = CategoricalDomain(invalid_value_treatment=ivt)\n",
    "            dom.fit(trn_d[col], name=col)\n",
    "            op_lst.append(dom)\n",
    "            op_lst.extend(prep)\n",
    "        elif col in invalid_ftr:\n",
    "            dom = OrdinalDomain(field_usage_treatment=\"supplementary\")\n",
    "            dom.fit(trn_d[col], name=col)\n",
    "            op_lst.append(dom)\n",
    "        elif col in conti_ftr:\n",
    "            ivt = missing_ivt(mis_val, col)\n",
    "            if 'as_missing' == ivt and 'mean' == mis_val[col]:\n",
    "                prep.append(Imputer().fit(trn_d[col]))\n",
    "            prep.append(continuous_feature_transform(ftr_trf, col).fit(trn_d[col]))\n",
    "            dom = ContinuousDomain(invalid_value_treatment=ivt)\n",
    "            dom.fit(trn_d[col], name=col)\n",
    "            op_lst.append(dom)\n",
    "            op_lst.extend(prep)\n",
    "        else:\n",
    "            op_lst = None\n",
    "        c_map.append(([col], op_lst))\n",
    "    return DataFrameMapper(c_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def dataMapperPrepare(trn_d, parent_dfm, target_col=None):\n",
    "    \"\"\"\n",
    "    datamapper prepare\n",
    "    trn_d: train data\n",
    "    parent_dfm: parent model datamapper\n",
    "    target_col: specify target_col or target_col will be infered according to mapper\n",
    "    \"\"\"\n",
    "    # none_ftr check\n",
    "    none_ftr = [feature[0] for feature, mapper in parent_dfm.features\n",
    "                if mapper is None]\n",
    "    if target_col:\n",
    "        data = move_target_last(trn_d, target_col)\n",
    "    elif len(none_ftr) == 1 and not target_col:\n",
    "        target_col = none_ftr[0]\n",
    "        data = move_target_last(trn_d, target_col)\n",
    "    else:\n",
    "        raise Exception('df_mapper error')\n",
    "\n",
    "    # domain ftr check\n",
    "    invalid_ftr = [col for feature, mapper in parent_dfm.features\n",
    "                   if mapper and mapper.domain_ == 'ordinaldomain'\n",
    "                   for col in feature]\n",
    "    categ_ftr = [col for feature, mapper in parent_dfm.features\n",
    "                 if mapper and mapper.domain_ == 'categoricaldomain'\n",
    "                 for col in feature]\n",
    "    conti_ftr = [col for feature, mapper in parent_dfm.features\n",
    "                 if mapper and mapper.domain_ == 'continuousdomain'\n",
    "                 for col in feature]\n",
    "\n",
    "    domain_ftr = set(invalid_ftr) | set(categ_ftr) | set(conti_ftr)\n",
    "    if set(trn_d.columns) - set(none_ftr) - domain_ftr:\n",
    "        raise DataMapperError(\"\"\"\n",
    "        datamapper consistency error:\n",
    "        col = {}\n",
    "        none = {}\n",
    "        invalid = {}\n",
    "        categorical = {}\n",
    "        continuous = {}\n",
    "        \"\"\".format(set(trn_d.columns), set(none_ftr),\n",
    "                   set(invalid_ftr), set(categ_ftr), set(conti_ftr)))\n",
    "\n",
    "    for ftr in categ_ftr:\n",
    "        data[ftr] = data[ftr].str.encode('utf-8')\n",
    "\n",
    "    # missing_value_check\n",
    "    def isreal(mapper):\n",
    "        return mapper.domain_ == 'categoricaldomain' or mapper.domain_ == 'continuousdomain'\n",
    "\n",
    "    missing_value_treatment = [(name, mapper.missing_value_treatment)\n",
    "                               for (name, mapper) in parent_dfm.features\n",
    "                               if mapper and isreal(mapper)]\n",
    "    missing_value_replacement = [(name, mapper.missingValueReplacement)\n",
    "                                 for (name, mapper) in parent_dfm.features\n",
    "                                 if mapper and isreal(mapper)]\n",
    "    ftr_trf = {col: mapper.steps[-1][0] for (name, mapper) in parent_dfm.features\n",
    "               for col in name if mapper and col in domain_ftr}\n",
    "    mis_val = {}\n",
    "\n",
    "    # missing value treatment\n",
    "    for col_treatment, col_replacement in zip(missing_value_treatment, missing_value_replacement):\n",
    "        name, treatment = col_treatment\n",
    "        _, defaultVal = col_replacement\n",
    "        if not isinstance(name, list):\n",
    "            cols = [name]\n",
    "        else:\n",
    "            cols = name\n",
    "        if treatment:\n",
    "            for col in cols:\n",
    "                data[col] = data[col].fillna(defaultVal)\n",
    "                mis_val[col] = (treatment, defaultVal)\n",
    "\n",
    "    dfm = dataMapperBuilder(data, categ_ftr, conti_ftr, invalid_ftr, mis_val, ftr_trf)\n",
    "    return data, dfm, target_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def buildTrainMapper(data, target, id_column=None):\n",
    "    (transformed, categorical_features,\n",
    "     continueous_features, invalid_feature) = prepare_for_training(data, target, id_column)\n",
    "    datamapper = dataMapperBuilder(transformed, categorical_features, continueous_features)\n",
    "    return transformed, datamapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def prepare_for_training(data, target, id_column=None):\n",
    "    \"\"\"\n",
    "    prepare_for_training shortcuts: using pandas infer\n",
    "    data: train data\n",
    "    target: label\n",
    "    id_column: drop columns\n",
    "    \"\"\"\n",
    "    transformed = data.copy()\n",
    "\n",
    "    tmp = transformed.pop(target)\n",
    "    transformed.insert(transformed.shape[1], target, tmp)\n",
    "\n",
    "    if id_column and id_column in transformed.columns:\n",
    "        invalid_features = transformed[id_column]\n",
    "        transformed.drop(id_column, axis=1, inplace=True)\n",
    "\n",
    "    contineous_describe = transformed.describe()\n",
    "    non_features = set([target]) | set(invalid_features)\n",
    "    continueous_features = set(contineous_describe.columns) - non_features\n",
    "    categorical_features = set(transformed.columns) - set(continueous_features) - non_features\n",
    "    for feature in categorical_features:\n",
    "        transformed[feature] = transformed[feature].astype('category')\n",
    "    for feature in continueous_features:\n",
    "        transformed[feature] = transformed[feature].astype('float32')\n",
    "\n",
    "    return transformed, categorical_features, continueous_features, invalid_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python2/lib/python2.7/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    a = pd.DataFrame([[\"1\", 2, 3, 1], [\"4\", 5, 6, 1], [\"7\", 8, 9, 1]])\n",
    "    dfm = dataMapperBuilder(a, [0], [1],\n",
    "                            invalid_ftr=[2],\n",
    "                            mis_val={0: ('asMode', \"1\")},\n",
    "                            ftr_trf={1: 'log1p'})\n",
    "    pd.DataFrame(dfm.features)\n",
    "    tmp = pd.DataFrame(dfm.features)\n",
    "    treat = tmp.ix[1, 1].steps[1][0]\n",
    "\n",
    "    trn_d = pd.concat([a, pd.DataFrame([[np.nan, 11, 12]])], axis=0)\n",
    "    assert dataMapperPrepare(trn_d, dfm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
