{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/opt/conda/lib/python3.5/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# <api>\n",
    "from sklearn2pmml.decoration import ContinuousDomain, CategoricalDomain\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import Imputer, Normalizer\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer, OneHotEncoder\n",
    "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "import base64\n",
    "from six import string_types\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "try:\n",
    "    from exceptions import Exception\n",
    "except:\n",
    "    pass\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "class DataMapperError(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def is_binary(ftr_vlst):\n",
    "    uniq_sp = ftr_vlst.unique()\n",
    "    uniq_sp = uniq_sp[~pd.isnull(uniq_sp)]\n",
    "    return 2 == len(uniq_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def onehot_encoder_with_missing(trn_series, na='CreditX-NA'):\n",
    "    unary = (trn_series.unique()[0] == 1)\n",
    "    binary_with_na = is_binary(trn_series) and na in set(trn_series)\n",
    "    if unary or binary_with_na:\n",
    "        return LabelEncoder()\n",
    "    else:\n",
    "        return LabelBinarizer()\n",
    "\n",
    "\n",
    "def continuous_feature_transform(ftr_trf, col):\n",
    "    prep = None\n",
    "    if col in ftr_trf:\n",
    "        if isinstance(ftr_trf[col], string_types):\n",
    "            prep = FtrTransFunc(ftr_trf[col]).method\n",
    "        elif isinstance(ftr_trf[col], FtrTransFunc):\n",
    "            prep = ftr_trf[col].method\n",
    "        else:\n",
    "            raise DataMapperError('invalid feature transformer: {}'.format(ftr_trf[col]))\n",
    "    return prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "class MisValFunc(Enum):\n",
    "    \"\"\"\n",
    "    AS_VALUE:  means set as new value (both training and serving)\n",
    "    AS_MEAN:  means set as asMean (both training and serving)\n",
    "    AS_MEDIAN:  means set as asMedian (both training and serving)\n",
    "    AS_MODE:  means set as mode (both training and serving)\n",
    "    AS_IS:  means drop_row during training but asIs during serving(warning)\n",
    "    DROP_ROW:  means drop_row during training but asMean/asMode during serving\n",
    "    DEFAULT: drop_row during training but asMean/asMode during serving\n",
    "    \"\"\"\n",
    "    AS_VALUE = 'asValue'\n",
    "    AS_MEAN = 'asMean'\n",
    "    AS_MEDIAN = 'asMedian'\n",
    "    AS_MODE = 'asMode'\n",
    "    AS_IS = 'asIs'\n",
    "    DROP_ROW = 'dropRow'\n",
    "    DEFAULT = 'dropRow'\n",
    "\n",
    "    def categoricalTransform(self,\n",
    "                             series,\n",
    "                             missing_value_replacement=None,\n",
    "                             invalid_value_treatment='asMissing'):\n",
    "        if self is MisValFunc.AS_VALUE:\n",
    "            missing_value_treatment = 'asValue'\n",
    "            if missing_value_replacement is None:\n",
    "                raise DataMapperError('no missing_value_replacement for {}'.format(self))\n",
    "        elif self is MisValFunc.AS_MODE:\n",
    "            missing_value_treatment = 'asMode'\n",
    "        elif self is MisValFunc.AS_MEAN or self is MisValFunc.AS_MEDIAN:\n",
    "            missing_value_treatment = 'asMode'\n",
    "        elif self is MisValFunc.DROP_ROW or self is MisValFunc.DEFAULT:\n",
    "            missing_value_treatment = 'asMode'\n",
    "        elif self is MisValFunc.AS_IS:\n",
    "            missing_value_treatment = 'asIs'\n",
    "        else:\n",
    "            raise DataMapperError('missing_value_treatment for categorical {}'.format(self))\n",
    "        domain = CategoricalDomain(invalid_value_treatment=invalid_value_treatment,\n",
    "                                   missing_value_treatment=missing_value_treatment,\n",
    "                                   missing_value_replacement=missing_value_replacement)\n",
    "        encoder = self.encoder(series)\n",
    "        return [domain, encoder] if encoder else [domain]\n",
    "\n",
    "    def continuousTransform(self,\n",
    "                            missing_value_replacement=None,\n",
    "                            invalid_value_treatment='asIs'):\n",
    "        if self is MisValFunc.AS_VALUE:\n",
    "            missing_value_treatment = 'asValue'\n",
    "            if missing_value_replacement is None:\n",
    "                raise DataMapperError('no missing_value_replacement for {}'.format(self))\n",
    "        elif self is MisValFunc.AS_MEDIAN or self is MisValFunc.AS_MODE:\n",
    "            missing_value_treatment = 'asMedian'\n",
    "        elif self is MisValFunc.AS_MEAN:\n",
    "            missing_value_treatment = 'asMean'\n",
    "        elif self is MisValFunc.AS_IS or self is MisValFunc.DROP_ROW or self is MisValFunc.DEFAULT:\n",
    "            missing_value_treatment = 'asMean'\n",
    "        else:\n",
    "            raise DataMapperError('missing_value_treatment for continuous {}'.format(self))\n",
    "        domain = ContinuousDomain(invalid_value_treatment=invalid_value_treatment,\n",
    "                                  missing_value_treatment=missing_value_treatment,\n",
    "                                  missing_value_replacement=missing_value_replacement)\n",
    "        imputer = self.imputer()\n",
    "        return [domain, imputer] if imputer else [domain]\n",
    "\n",
    "    def encoder(self, series):\n",
    "        return onehot_encoder_with_missing(series)\n",
    "\n",
    "    def imputer(self, missing_values='NaN'):\n",
    "        if self is MisValFunc.DROP_ROW or self is MisValFunc.AS_MEAN:\n",
    "            return Imputer(missing_values=missing_values, strategy='median')\n",
    "        elif self is MisValFunc.AS_MODE:\n",
    "            return Imputer(missing_values=missing_values, strategy='most_frequent')\n",
    "        elif self is MisValFunc.AS_MEAN:\n",
    "            return Imputer(missing_values=missing_values, strategy='mean')\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def apply(self, ftr, data, val=None):\n",
    "        if self is MisValFunc.DEFAULT:\n",
    "            set_default_value(data, ftr, val)\n",
    "            return data\n",
    "        elif self is MisValFunc.AS_MEAN:\n",
    "            set_mean(data, ftr)\n",
    "            return data\n",
    "        elif self is MisValFunc.AS_MEDIAN:\n",
    "            set_median(data, ftr)\n",
    "            return data\n",
    "        elif self is MisValFunc.DROP_ROW:\n",
    "            drop_row(data, ftr)\n",
    "            return data\n",
    "        elif self is MisValFunc.AS_VALUE:\n",
    "            newVal = set_as_new_class(data, ftr)\n",
    "            return self.value, newVal\n",
    "        else:\n",
    "            raise NotImplementedError(\"unsupported missing value transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "class FtrTransFunc(Enum):\n",
    "    MIN_MAX_SCALER = 'MinMaxScaler'\n",
    "    STANDARD_SCALER = 'StandardScaler'\n",
    "    MAX_ABS_SCALER = 'MaxAbsScaler'\n",
    "    NORMALIZER = 'Normalizer'\n",
    "    BINARIZER = 'Binarizer'\n",
    "    ONE_HOT_ENCODER = 'OneHotEncoder'\n",
    "    LOG1P = 'log1p'\n",
    "    LOG = 'log'\n",
    "\n",
    "    @property\n",
    "    def method(self):\n",
    "        if self is FtrTransFunc.MIN_MAX_SCALER:\n",
    "            return MinMaxScaler(copy=False)\n",
    "        elif self is FtrTransFunc.STANDARD_SCALER:\n",
    "            return StandardScaler(copy=False)\n",
    "        elif self is FtrTransFunc.MAX_ABS_SCALER:\n",
    "            return MaxAbsScaler(copy=False)\n",
    "        elif self is FtrTransFunc.NORMALIZER:\n",
    "            ft = FunctionTransformer(Normalizer(axis=0), False)\n",
    "            ft.name = self.name\n",
    "            return ft\n",
    "        elif self is FtrTransFunc.BINARIZER:\n",
    "            return LabelBinarizer(copy=False)\n",
    "        elif self is FtrTransFunc.ONE_HOT_ENCODER:\n",
    "            return OneHotEncoder()\n",
    "        elif self is FtrTransFunc.LOG1P:\n",
    "            ft = FunctionTransformer(np.log1p, False)\n",
    "            ft.name = self.name\n",
    "            return ft\n",
    "        elif self is FtrTransFunc.LOG:\n",
    "            ft = FunctionTransformer(np.log, False)\n",
    "            ft.name = self.name\n",
    "            return ft\n",
    "        else:\n",
    "            raise NotImplementedError(\"unsupported feature transformer\")\n",
    "\n",
    "    def apply(self, series):\n",
    "        after = self.method.fit_transform(series)\n",
    "        if 'category' != series.dtype.name and contain_nan(after):\n",
    "            raise Exception('feature contains nan when transformed by ' + self)\n",
    "        if 'category' != series.dtype.name and contain_inf(after):\n",
    "            raise Exception('feature contains inf when transformed by ' + self)\n",
    "        return after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def b64_file_data(fig_path):\n",
    "    fig_data = None\n",
    "    with open(fig_path, 'r') as infile:\n",
    "        fig_data = infile.read()\n",
    "    return base64.b64encode(fig_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def drop_row(data, ftr):\n",
    "    data.dropna(how='any', subset=[ftr], inplace=True)\n",
    "\n",
    "\n",
    "def contain_nan(series):\n",
    "    where = np.where(np.isnan(series))\n",
    "    return 0 != len(where[0])\n",
    "\n",
    "\n",
    "def contain_inf(series):\n",
    "    where = np.where(np.isinf(series))\n",
    "    return 0 != len(where[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def set_as_new_class(data, ftr):\n",
    "    uniq_v = data[ftr].unique()\n",
    "    uniq_v = uniq_v[~pd.isnull(uniq_v)]\n",
    "    if 0 == len(uniq_v):\n",
    "        raise Exception('all values of ' + ftr + ' are nan')\n",
    "\n",
    "    # maybe need more check for the data type\n",
    "    v = uniq_v[0]\n",
    "    if isinstance(v, str):\n",
    "        new_v = ftr + '_NA'\n",
    "    elif isinstance(v, (float, int)):\n",
    "        new_v = uniq_v.astype('float32').max() + 1\n",
    "        data[ftr] = data[ftr].astype('float32')\n",
    "    else:\n",
    "        raise Exception('categorical value is string or numerical?')\n",
    "    set_default_value(data, ftr, new_v)\n",
    "    data[ftr] = data[ftr].astype('category')\n",
    "    return new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def set_default_value(data, ftr, v):\n",
    "    if v is None:\n",
    "        raise Exception('value is None')\n",
    "\n",
    "    old_type = 'category'\n",
    "    if 'category' == data[ftr].dtype.name:\n",
    "        data[ftr] = data[ftr].astype('object', copy=True)\n",
    "        v = str(v)\n",
    "    else:\n",
    "        data[ftr] = data[ftr].astype('float32', copy=True)\n",
    "        v = float(v)\n",
    "        old_type = 'float32'\n",
    "\n",
    "    data[ftr].fillna(v, inplace=True)\n",
    "    data[ftr] = data[ftr].astype(old_type, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def set_mean(data, ftr):\n",
    "    series = data[ftr]\n",
    "    tmp = Imputer(axis=1, strategy='median').fit_transform(series)\n",
    "    data[ftr].update(pd.Series(tmp[0]))\n",
    "\n",
    "\n",
    "def set_median(data, ftr):\n",
    "    series = data[ftr]\n",
    "    tmp = Imputer(axis=1, strategy='median').fit_transform(series)\n",
    "    data[ftr].update(pd.Series(tmp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def move_target_last(data, target_col):\n",
    "    reindex_col = [c for c in data.columns]\n",
    "    if target_col not in reindex_col:\n",
    "        return data\n",
    "    reindex_col.remove(target_col)\n",
    "    reindex_col.append(target_col)\n",
    "    return data.reindex_axis(reindex_col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def dataMapperBuilder(trn_d, categ_ftr, conti_ftr,\n",
    "                      mis_val=None, ftr_trf=None):\n",
    "    \"\"\"\n",
    "    build dataFrameMapper according to colume type\n",
    "    trn_d: traning data in DataFrame format\n",
    "    categ_ftr: categorical feature(to_dummies)\n",
    "    conti_ftr: continuous feature(feature transformer)\n",
    "    mis_val: missing value treatment\n",
    "    ftr_trf: feature transformer\n",
    "    \"\"\"\n",
    "    mis_val = mis_val if mis_val else {}\n",
    "    ftr_trf = ftr_trf if ftr_trf else {}\n",
    "    c_map = []\n",
    "    for col in trn_d.columns:\n",
    "        if col in categ_ftr:\n",
    "            # missing imputer\n",
    "            mis, val = mis_val.get(col, ('asMean', None))\n",
    "            misValFunc = MisValFunc(mis)\n",
    "            op_lst = misValFunc.categoricalTransform(mis_val[col],\n",
    "                                                     missing_value_replacement=val)\n",
    "        elif col in conti_ftr:\n",
    "            # missing imputer\n",
    "            mis, val = mis_val.get(col, ('asMean', None))\n",
    "            misValFunc = MisValFunc(mis)\n",
    "            op_lst = misValFunc.continuousTransform(missing_value_replacement=val)\n",
    "            # feature transform\n",
    "            ftr_trans = continuous_feature_transform(ftr_trf, col)\n",
    "            if ftr_trans:\n",
    "                op_lst.append(ftr_trans)\n",
    "        else:\n",
    "            op_lst = None\n",
    "        c_map.append((col, op_lst))\n",
    "    return DataFrameMapper(c_map, df_out=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def dataMapperPrepare(trn_d, parent_dfm, target_col=None):\n",
    "    \"\"\"\n",
    "    datamapper prepare\n",
    "    trn_d: train data\n",
    "    parent_dfm: parent model datamapper\n",
    "    target_col: target_col would be validated according to mapper\n",
    "    \"\"\"\n",
    "    # target check\n",
    "    target = [feature for feature, mapper in parent_dfm.features\n",
    "              if mapper is None]\n",
    "    if target_col and target:\n",
    "        if target_col in target:\n",
    "            raise DataMapperError('df_mapper target mismatch')\n",
    "        data = move_target_last(trn_d, target_col)\n",
    "    elif target and not target_col:\n",
    "        target_col = target[0]\n",
    "        data = move_target_last(trn_d, target_col)\n",
    "    else:\n",
    "        raise DataMapperError('df_mapper no target error')\n",
    "\n",
    "    # domain ftr check\n",
    "\n",
    "    categ_ftr = [feature for feature, mapper in parent_dfm.features\n",
    "                 if mapper and mapper.domain_ == 'categoricaldomain']\n",
    "    conti_ftr = [feature for feature, mapper in parent_dfm.features\n",
    "                 if mapper and mapper.domain_ == 'continuousdomain']\n",
    "\n",
    "    domain_ftr = set(categ_ftr) | set(conti_ftr)\n",
    "    if set(trn_d.columns) - set(target) - domain_ftr:\n",
    "        raise DataMapperError(\"\"\"\n",
    "        datamapper inconsistency:\n",
    "        col = {}\n",
    "        target = {}\n",
    "        categorical = {}\n",
    "        continuous = {}\n",
    "        \"\"\".format(set(trn_d.columns), set(target),\n",
    "                  set(categ_ftr), set(conti_ftr)))\n",
    "\n",
    "    # categ_ftr string encode\n",
    "    for ftr in categ_ftr:\n",
    "        data[ftr] = data[ftr].str.encode('utf-8')\n",
    "\n",
    "    missing_value_treatment = {name: mapper.missing_value_treatment\n",
    "                               for (name, mapper) in parent_dfm.features\n",
    "                               if mapper}\n",
    "    missing_value_replacement = {name: mapper.missingValueReplacement\n",
    "                                 for (name, mapper) in parent_dfm.features\n",
    "                                 if mapper}\n",
    "    ftr_trf = {name: mapper.steps[-1][0] for (name, mapper) in parent_dfm.features\n",
    "               if mapper and name in domain_ftr}\n",
    "    mis_val = {}\n",
    "\n",
    "    # missing value treatment\n",
    "    for col, treatment in missing_value_treatment.items():\n",
    "        defaultVal = missing_value_replacement[col]\n",
    "        if treatment:\n",
    "            data[col] = data[col].fillna(defaultVal)\n",
    "            mis_val[col] = (treatment, defaultVal)\n",
    "\n",
    "    dfm = dataMapperBuilder(data, categ_ftr, conti_ftr, mis_val, ftr_trf)\n",
    "    return data, dfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def buildTrainMapper(data, target, id_column=None):\n",
    "    (transformed, categorical_features,\n",
    "     continueous_features, invalid_feature) = prepare_for_training(data, target, id_column)\n",
    "    datamapper = dataMapperBuilder(transformed, categorical_features, continueous_features)\n",
    "    return transformed, datamapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def prepare_for_training(data, target, id_column=None):\n",
    "    \"\"\"\n",
    "    prepare_for_training shortcuts: using pandas infer\n",
    "    data: train data\n",
    "    target: label\n",
    "    id_column: drop columns\n",
    "    \"\"\"\n",
    "    transformed = data.copy()\n",
    "\n",
    "    tmp = transformed.pop(target)\n",
    "    transformed.insert(transformed.shape[1], target, tmp)\n",
    "\n",
    "    if id_column and id_column in transformed.columns:\n",
    "        invalid_features = transformed[id_column]\n",
    "        transformed.drop(id_column, axis=1, inplace=True)\n",
    "\n",
    "    contineous_describe = transformed.describe()\n",
    "    non_features = set([target]) | set(invalid_features)\n",
    "    continueous_features = set(contineous_describe.columns) - non_features\n",
    "    categorical_features = set(transformed.columns) - set(continueous_features) - non_features\n",
    "    for feature in categorical_features:\n",
    "        transformed[feature] = transformed[feature].astype('category')\n",
    "    for feature in continueous_features:\n",
    "        transformed[feature] = transformed[feature].astype('float32')\n",
    "\n",
    "    return transformed, categorical_features, continueous_features, invalid_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
